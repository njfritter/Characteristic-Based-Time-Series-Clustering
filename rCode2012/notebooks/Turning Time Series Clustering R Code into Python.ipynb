{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Code\n",
    "## https://www.r-bloggers.com/measuring-time-series-characteristics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Quick and dirty implementation of characteristic based time series clustering as seen in: \n",
    "# https://www.r-bloggers.com/measuring-time-series-characteristics/\n",
    "library(forecast)\n",
    "\n",
    "# Function to find the frequency of the time series data inputted\n",
    "find.freq <- function(x)\n",
    "{\n",
    "  n <- length(x)\n",
    "  spec <- spec.ar(c(na.contiguous(x)),plot=FALSE)\n",
    "  if(max(spec$spec)>10) # Arbitrary threshold chosen by trial and error.\n",
    "  {\n",
    "    period <- round(1/spec$freq[which.max(spec$spec)])\n",
    "    if(period==Inf) # Find next local maximum\n",
    "    {\n",
    "      j <- which(diff(spec$spec)>0)\n",
    "      if(length(j)>0)\n",
    "      {\n",
    "        nextmax <- j[1] + which.max(spec$spec[j[1]:500])\n",
    "        if(nextmax <= length(spec$freq))\n",
    "          period <- round(1/spec$freq[nextmax])\n",
    "        else\n",
    "          period <- 1\n",
    "      }\n",
    "      else\n",
    "        period <- 1\n",
    "    }\n",
    "  }\n",
    "  else\n",
    "    period <- 1\n",
    "  \n",
    "  return(period)\n",
    "}\n",
    "\n",
    "\n",
    "# Function that decomposes the data into trend and seasonal components\n",
    "decomp <- function(x,transform=TRUE)\n",
    "{\n",
    "  require(forecast)\n",
    "  # Transform series\n",
    "  if(transform & min(x,na.rm=TRUE) >= 0)\n",
    "  {\n",
    "    lambda <- BoxCox.lambda(na.contiguous(x))\n",
    "    x <- BoxCox(x,lambda)\n",
    "  }\n",
    "  else\n",
    "  {\n",
    "    lambda <- NULL\n",
    "    transform <- FALSE\n",
    "  }\n",
    "  # Seasonal data\n",
    "  if(frequency(x)>1)\n",
    "  {\n",
    "    x.stl <- stl(x,s.window=\"periodic\",na.action=na.contiguous)\n",
    "    trend <- x.stl$time.series[,2]\n",
    "    season <- x.stl$time.series[,1]\n",
    "    remainder <- x - trend - season\n",
    "  }\n",
    "  else #Nonseasonal data\n",
    "  {\n",
    "    require(mgcv)\n",
    "    tt <- 1:length(x)\n",
    "    trend <- rep(NA,length(x))\n",
    "    trend[!is.na(x)] <- fitted(gam(x ~ s(tt)))\n",
    "    season <- NULL\n",
    "    remainder <- x - trend\n",
    "  }\n",
    "  return(list(x=x,trend=trend,season=season,remainder=remainder,\n",
    "              transform=transform,lambda=lambda))\n",
    "}\n",
    "\n",
    "# Functions to map all the features onto a [0,1] scale\n",
    "# f1 maps [0,infinity) to [0,1]\n",
    "f1 <- function(x,a,b)\n",
    "{\n",
    "  eax <- exp(a*x)\n",
    "  if (eax == Inf)\n",
    "    f1eax <- 1\n",
    "  else\n",
    "    f1eax <- (eax-1)/(eax+b)\n",
    "  return(f1eax)\n",
    "}\n",
    "\n",
    "# f2 maps [0,1] onto [0,1]\n",
    "f2 <- function(x,a,b)\n",
    "{\n",
    "  eax <- exp(a*x)\n",
    "  ea <- exp(a)\n",
    "  return((eax-1)/(eax+b)*(ea+b)/(ea-1))\n",
    "}\n",
    "\n",
    "# Finally, calculate measures\n",
    "library(tseries)\n",
    "library(fracdiff)\n",
    "measures <- function(x)\n",
    "{\n",
    "  require(forecast)\n",
    "  \n",
    "  N <- length(x)\n",
    "  freq <- find.freq(x)\n",
    "  fx <- c(frequency=(exp((freq-1)/50)-1)/(1+exp((freq-1)/50)))\n",
    "  x <- ts(x,f=freq)\n",
    "  \n",
    "  # Decomposition\n",
    "  decomp.x <- decomp(x)\n",
    "  \n",
    "  # Adjust data\n",
    "  if(freq > 1)\n",
    "    fits <- decomp.x$trend + decomp.x$season\n",
    "  else # Nonseasonal data\n",
    "    fits <- decomp.x$trend\n",
    "  adj.x <- decomp.x$x - fits + mean(decomp.x$trend, na.rm=TRUE)\n",
    "  \n",
    "  # Backtransformation of adjusted data\n",
    "  if(decomp.x$transform)\n",
    "    tadj.x <- InvBoxCox(adj.x,decomp.x$lambda)\n",
    "  else\n",
    "    tadj.x <- adj.x\n",
    "  \n",
    "  # Trend and seasonal measures\n",
    "  v.adj <- var(adj.x, na.rm=TRUE)\n",
    "  if(freq > 1)\n",
    "  {\n",
    "    detrend <- decomp.x$x - decomp.x$trend\n",
    "    deseason <- decomp.x$x - decomp.x$season\n",
    "    trend <- ifelse(var(deseason,na.rm=TRUE) < 1e-10, 0, \n",
    "                    max(0,min(1,1-v.adj/var(deseason,na.rm=TRUE))))\n",
    "    season <- ifelse(var(detrend,na.rm=TRUE) < 1e-10, 0,\n",
    "                     max(0,min(1,1-v.adj/var(detrend,na.rm=TRUE))))\n",
    "  }\n",
    "  else #Nonseasonal data\n",
    "  {\n",
    "    trend <- ifelse(var(decomp.x$x,na.rm=TRUE) < 1e-10, 0,\n",
    "                    max(0,min(1,1-v.adj/var(decomp.x$x,na.rm=TRUE))))\n",
    "    season <- 0\n",
    "  }\n",
    "  \n",
    "  m <- c(fx,trend,season)\n",
    "  \n",
    "  # Measures on original data\n",
    "  xbar <- mean(x,na.rm=TRUE)\n",
    "  s <- sd(x,na.rm=TRUE)\n",
    "  \n",
    "  # Serial correlation\n",
    "  Q <- Box.test(x,lag=10)$statistic/(N*10)\n",
    "  fQ <- f2(Q,7.53,0.103)\n",
    "  \n",
    "  # Nonlinearity\n",
    "  p <- terasvirta.test(na.contiguous(x))$statistic\n",
    "  fp <- f1(p,0.069,2.304)\n",
    "  \n",
    "  # Skewness\n",
    "  s <- abs(mean((x-xbar)^3,na.rm=TRUE)/s^3)\n",
    "  fs <- f1(s,1.510,5.993)\n",
    "  \n",
    "  # Kurtosis\n",
    "  k <- mean((x-xbar)^4,na.rm=TRUE)/s^4\n",
    "  fk <- f1(k,2.273,11567)\n",
    "  \n",
    "  # Hurst=d+0.5 where d is fractional difference.\n",
    "  H <- fracdiff(na.contiguous(x),0,0)$d + 0.5\n",
    "  \n",
    "  # Lyapunov Exponent\n",
    "  if(freq > N-10)\n",
    "    stop(\"Insufficient data\")\n",
    "  Ly <- numeric(N-freq)\n",
    "  for(i in 1:(N-freq))\n",
    "  {\n",
    "    idx <- order(abs(x[i] - x))\n",
    "    idx <- idx[idx < (N-freq)]\n",
    "    j <- idx[2]\n",
    "    Ly[i] <- log(abs((x[i+freq] - x[j+freq])/(x[i]-x[j])))/freq\n",
    "    if(is.na(Ly[i]) | Ly[i]==Inf | Ly[i]==-Inf)\n",
    "      Ly[i] <- NA\n",
    "  }\n",
    "  Lyap <- mean(Ly,na.rm=TRUE)\n",
    "  fLyap <- exp(Lyap)/(1+exp(Lyap))\n",
    "  \n",
    "  m <- c(m,fQ,fp,fs,fk,H,fLyap)\n",
    "  \n",
    "  # Measures on adjusted data\n",
    "  xbar <- mean(tadj.x, na.rm=TRUE)\n",
    "  s <- sd(tadj.x, na.rm=TRUE)\n",
    "  \n",
    "  # Serial\n",
    "  Q <- Box.test(adj.x,lag=10)$statistic/(N*10)\n",
    "  fQ <- f2(Q,7.53,0.103)\n",
    "  \n",
    "  # Nonlinearity\n",
    "  p <- terasvirta.test(na.contiguous(adj.x))$statistic\n",
    "  fp <- f1(p,0.069,2.304)\n",
    "  \n",
    "  # Skewness\n",
    "  s <- abs(mean((tadj.x-xbar)^3,na.rm=TRUE)/s^3)\n",
    "  fs <- f1(s,1.510,5.993)\n",
    "  \n",
    "  # Kurtosis\n",
    "  k <- mean((tadj.x-xbar)^4,na.rm=TRUE)/s^4\n",
    "  fk <- f1(k,2.273,11567)\n",
    "  \n",
    "  m <- c(m,fQ,fp,fs,fk)\n",
    "  names(m) <- c(\"frequency\", \"trend\",\"seasonal\",\n",
    "                \"autocorrelation\",\"non-linear\",\"skewness\",\"kurtosis\",\n",
    "                \"Hurst\",\"Lyapunov\",\n",
    "                \"dc autocorrelation\",\"dc non-linear\",\"dc skewness\",\"dc kurtosis\")\n",
    "  \n",
    "  return(m)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now in Python!\n",
    "\n",
    "## First find frequency (this unfortunately doesn't work yet, including for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import periodogram\n",
    "def find_freq(x):\n",
    "    # Use an iterative function to automagically determine the frequency of the time series data\n",
    "    # Takes in a single column of a pandas DataFrame as a univariate time series\n",
    "    \n",
    "    n = len(x)\n",
    "    # Now estimate the spectral density of the time series via AR fit\n",
    "    # Two ways: numpy fft method or scipy signal method\n",
    "    # Method #1: numpy fft method (https://stackoverflow.com/questions/15382076/plotting-power-spectrum-in-python)\n",
    "    '''\n",
    "    pow_np = np.abs(np.fft.fft(x))**2\n",
    "    time_step = 1 / n\n",
    "    freqs = np.fft.fftfreq(n, time_step)\n",
    "    idx = np.argsort(freqs)\n",
    "    plt.plot(freqs[idx], ps[idx])\n",
    "    '''\n",
    "    \n",
    "    # Method #2: scipy signal method via density or spectrum (takes an optional second frequency parameter)\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.periodogram.html\n",
    "    #f, pow_scipy = signal.periodogram(x,scaling='density')\n",
    "    f, pow_scipy = periodogram(x,scaling='spectrum')\n",
    "    \n",
    "    # Iterate through frequencies \n",
    "    freq = f\n",
    "    power = pow_scipy\n",
    "    if max(power) > 10: # Arbitrary threshold chosen by trial and error.\n",
    "        # The power might be a huge number way of out the index bounds, so pick max index if so\n",
    "        power_idx = min(int(round(max(power))),(len(power)-1))\n",
    "        freq_idx = min(int(round(power[power_idx])),len(freq)-1)\n",
    "        period = 1/freq[freq_idx]\n",
    "        # If period is infinity, find next local maximum\n",
    "        if period == np.Inf:\n",
    "            j = pd.Series(power).diff() > 0\n",
    "            if len(j) > 0:\n",
    "                nextmax = j[1] + power[int(round(max(power[1:])))]\n",
    "                if (nextmax <= len(freq)):\n",
    "                    period = int(round(1/freq[int(round(nextmax))]))\n",
    "                else:\n",
    "                    period = 1\n",
    "            else:\n",
    "                period = 1\n",
    "        else:\n",
    "            period = int(round(period))\n",
    "    else:\n",
    "        period = 1\n",
    "    \n",
    "    return period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In R as well (since I can't get it to work in Python)\n",
    "\n",
    "#### This requires some manual work before these code blocks will work\n",
    "1. Set R_HOME variable (can be found by typing \".Library\" into R Studio)\n",
    "2. The output from the above is the \"library\" subdirectory of the R_HOME directory established by R Studio\n",
    "3. Take the output minus \"/library\" at the end, and save this to R_HOME in a terminal\n",
    "4. Run pip3 install rpy2 (connects to R_HOME at install)\n",
    "\n",
    "Will make an init.sh file to run all the necessary steps for this + other manual setup steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrbase.set_seed(123) # reproducibility seed\\nx = r.ts(r.rnorm(n=10)) # simulate the time series\\nprint(x)\\n\\n[-0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\\n  0.46091621 -1.26506123 -0.68685285 -0.44566197]\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sources: https://stackoverflow.com/questions/17573988/r-home-error-with-rpy2\n",
    "# https://stackoverflow.com/questions/47585718/rpy2-installed-but-wont-run-packages\n",
    "# https://stackoverflow.com/questions/24880493/how-to-find-out-r-library-location-in-mac-osx/24880594\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import r, pandas2ri, numpy2ri, Formula\n",
    "from rpy2.robjects.vectors import IntVector,FloatVector\n",
    "# Load necessary R packages\n",
    "rtseries = importr('tseries')\n",
    "rbase = importr('base')\n",
    "rstats = importr('stats')\n",
    "rfracdiff=importr('fracdiff')\n",
    "rutils=importr('utils')\n",
    "rmgcv=importr('mgcv')\n",
    "\n",
    "# \"Activate\" pandas2ri and numpy2ri\n",
    "#pandas2ri.activate()\n",
    "#numpy2ri.activate()\n",
    "\n",
    "# For some reason, running the above commands blocks me from being able to create R time series objects \n",
    "# Trying to create time series objects just leads to numpy arrays being created\n",
    "# https://www.r-bloggers.com/using-r-in-python-for-statistical-learning-data-science-2/\n",
    "# Example output when the below commands are NOT used:\n",
    "'''\n",
    "rbase.set_seed(123) # reproducibility seed\n",
    "x = r.ts(r.rnorm(n=10)) # simulate the time series\n",
    "print(x)\n",
    "\n",
    "Time Series:\n",
    "Start = 1 \n",
    "End = 10 \n",
    "Frequency = 1 \n",
    " [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n",
    " [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n",
    "'''\n",
    "\n",
    "# Example output when the below commands are used:\n",
    "'''\n",
    "rbase.set_seed(123) # reproducibility seed\n",
    "x = r.ts(r.rnorm(n=10)) # simulate the time series\n",
    "print(x)\n",
    "\n",
    "[-0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n",
    "  0.46091621 -1.26506123 -0.68685285 -0.44566197]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series:\n",
      "Start = 1 \n",
      "End = 10 \n",
      "Frequency = 1 \n",
      " [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n",
      " [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rbase.set_seed(123) # reproducibility seed\n",
    "x = r.ts(r.rnorm(n=10)) # simulate the time series\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas2ri.activate()\n",
    "#numpy2ri.activate()\n",
    "#print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series:\n",
      "Start = 1 \n",
      "End = 10 \n",
      "Frequency = 1 \n",
      " [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n",
      " [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rbase.set_seed(123) # reproducibility seed\n",
    "y = r.ts(r.rnorm(n=10)) # simulate the time series\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"freq\"   \"spec\"   \"coh\"    \"phase\"  \"n.used\" \"series\" \"method\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = r.ts(r.rnorm(n=30)) # simulate the time series\n",
    "spec = rstats.spec_ar(x,plot=False)\n",
    "print(spec.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RObjectMixin.r_repr of R object with classes: ('ts',) mapped to:\n",
       "[-0.560476, -0.230177, 1.558708, 0.070508, ..., 0.460916, -1.265061, -0.686853, -0.445662]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.r_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RObjectMixin.r_repr of R object with classes: ('numeric',) mapped to:\n",
       "[1.224082, 0.359814, 0.400771, 0.110683, ..., 0.497850, -1.966617, 0.701356, -0.472791]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = FloatVector(r.rnorm(n=10))\n",
    "z.r_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Function to find the frequency of the time series data inputted\\nfind.freq <- function(x)\\n{\\n  n <- length(x)\\n  spec <- spec.ar(c(na.contiguous(x)),plot=FALSE)\\n  if(max(spec$spec)>10) # Arbitrary threshold chosen by trial and error.\\n  {\\n    period <- round(1/spec$freq[which.max(spec$spec)])\\n    if(period==Inf) # Find next local maximum\\n    {\\n      j <- which(diff(spec$spec)>0)\\n      if(length(j)>0)\\n      {\\n        nextmax <- j[1] + which.max(spec$spec[j[1]:500])\\n        if(nextmax <= length(spec$freq))\\n          period <- round(1/spec$freq[nextmax])\\n        else\\n          period <- 1\\n      }\\n      else\\n        period <- 1\\n    }\\n  }\\n  else\\n    period <- 1\\n\\n  return(period)\\n}\\n'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_freq_r(x):\n",
    "    # Same as above function, but using the R code directly via rpy2\n",
    "    n = len(x)\n",
    "    #spec = rstats.spec_ar(r.ts(na_contiguous(x)),plot=False)\n",
    "    spec = rstats.spec_ar(x,plot=False)\n",
    "    # This returns a \"ListVector\" with the following items, which can be accessed via list indexing:\n",
    "    '''\n",
    "    x = r.ts(FloatVector(na_contiguous(analysis_data_by_user_id[100613640])))\n",
    "    spec = rstats.spec_ar(x,plot=False)\n",
    "    print(spec.names)\n",
    "    \n",
    "    [1] \"freq\"   \"spec\"   \"coh\"    \"phase\"  \"n.used\" \"series\" \"method\"\n",
    "    '''\n",
    "    spec_vals = spec[1]\n",
    "    spec_freq = spec[0]\n",
    "    #spec_vals = spec[np.where(spec.names == 'spec')[0].item()]\n",
    "    #spec_freq = spec[np.where(spec.names == 'freq')[0].item()]\n",
    "\n",
    "    if max(spec_vals) > 10:\n",
    "        #period <- round(1/spec$freq[which.max(spec$spec)])\n",
    "        denom = spec_freq[list(rbase.which_max(spec_vals))[0] - 1]\n",
    "        if denom != 0:\n",
    "            period = round(1/denom)\n",
    "        else: # Means we end up with infinity as a result, so evaluate additional code block\n",
    "            series = pd.Series(rbase.diff(spec_vals))\n",
    "            j = series[series > 0].reset_index(drop=True)\n",
    "            if len(j) > 0:\n",
    "                #nextmax <- j[1] + which.max(spec$spec[j[1]:500])\n",
    "                nextmax = j[0] + rbase.which_max(spec_vals[int(j[0]):500])\n",
    "                if nextmax.item() - 1 <= len(spec_freq):\n",
    "                    denom = spec_freq[round(nextmax.item() - 1)]\n",
    "                    if denom != 0:\n",
    "                        period = round(1/denom)\n",
    "                    else:\n",
    "                        period = 1\n",
    "                else:\n",
    "                    period = 1\n",
    "            else:\n",
    "                period = 1\n",
    "    else:\n",
    "        period = 1\n",
    "    \n",
    "    return int(period)\n",
    "    \n",
    "'''\n",
    "# Function to find the frequency of the time series data inputted\n",
    "find.freq <- function(x)\n",
    "{\n",
    "  n <- length(x)\n",
    "  spec <- spec.ar(c(na.contiguous(x)),plot=FALSE)\n",
    "  if(max(spec$spec)>10) # Arbitrary threshold chosen by trial and error.\n",
    "  {\n",
    "    period <- round(1/spec$freq[which.max(spec$spec)])\n",
    "    if(period==Inf) # Find next local maximum\n",
    "    {\n",
    "      j <- which(diff(spec$spec)>0)\n",
    "      if(length(j)>0)\n",
    "      {\n",
    "        nextmax <- j[1] + which.max(spec$spec[j[1]:500])\n",
    "        if(nextmax <= length(spec$freq))\n",
    "          period <- round(1/spec$freq[nextmax])\n",
    "        else\n",
    "          period <- 1\n",
    "      }\n",
    "      else\n",
    "        period <- 1\n",
    "    }\n",
    "  }\n",
    "  else\n",
    "    period <- 1\n",
    "\n",
    "  return(period)\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's do the decompose function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "#from statsmodels.gam.api import GLMGam, BSplines\n",
    "from statsmodels.gam.generalized_additive_model import GLMGam\n",
    "from statsmodels.gam.smooth_basis import BSplines,CyclicCubicSplines\n",
    "import statsmodels.api as sm\n",
    "#from statsmodels.gam.generalized_additive_model import GLMGam\n",
    "\n",
    "def na_contiguous(x):\n",
    "    # Recreate na.contiguous function in R since this is used frequently\n",
    "    # This takes a series object with a time index and finds the longest consecutive stretch of non-missing values\n",
    "    # https://stackoverflow.com/questions/41494444/pandas-find-longest-stretch-without-nan-values\n",
    "    # And then return the shortened dataframe with all non-null values\n",
    "    values = x.values \n",
    "    mask = np.concatenate(( [True], np.isnan(values), [True] ))  # Mask\n",
    "    start_stop = np.flatnonzero(mask[1:] != mask[:-1]).reshape(-1,2)   # Start-stop limits\n",
    "    start,stop = start_stop[(start_stop[:,1] - start_stop[:,0]).argmax()]  # Get max interval, interval limits\n",
    "    contiguous = x.iloc[start:stop]\n",
    "    return contiguous\n",
    "\n",
    "def decompose(x, transform = True):\n",
    "    # Decompose data into trend, seasonality and randomness\n",
    "    # Accepts a pandas series object with a datetime index\n",
    "    if (transform and min(x.dropna()) >= 0):\n",
    "        # Transforms data and finds the lambda that maximizes the log likelihood \n",
    "        # R version has above method and method that minimizes the coefficient of variation (\"guerrero\")\n",
    "        x_transformed, var_lambda = boxcox(na_contiguous(x),lmbda = None)\n",
    "        x_transformed = pd.Series(x_transformed,index=na_contiguous(x).index)\n",
    "    \n",
    "    else:\n",
    "        x_transformed = x\n",
    "        var_lambda = np.nan\n",
    "        transform = False\n",
    "    \n",
    "    # Seasonal data \n",
    "    # In R code, we find the number of samples per unit time below (should be 1 every time)\n",
    "    # Here I take the datetime index differences, take their inverses, and store in a list to be evaluated\n",
    "    # https://stackoverflow.com/questions/36583859/compute-time-difference-of-datetimeindex\n",
    "    idx = x_transformed.index\n",
    "    #samples = np.unique([int(1/(idx[n]-idx[n - 1]).days) for n in range(1,len(idx))])\n",
    "    # Filter out Nulls for this exercise\n",
    "    #samples = samples[~np.isnan(samples)]\n",
    "    #if len(samples) == 1 and samples.item() > 1:\n",
    "\n",
    "    # Just use the R code instead\n",
    "    # This is supposed to be \"> 1\" but all data results in a frequency of 1\n",
    "    # All frequency results in R equal 4, meaning this code block gets evaluated every time in R\n",
    "    # So this code block should always be evaluated as well\n",
    "    freq = rstats.frequency(r.ts(FloatVector(x_transformed)))\n",
    "    if list(freq)[0] == 1:\n",
    "        # Decompose\n",
    "        stl = sm.tsa.seasonal_decompose(na_contiguous(x_transformed))\n",
    "        #stl = rstats.stl(na_contiguous(x_transformed),s_window='periodic')\n",
    "        # When I try to use above function, I get this:\n",
    "        '''\n",
    "        R[write to console]: Error in (function (x, s.window, s.degree = 0, t.window = NULL, t.degree = 1,  : \n",
    "  series is not periodic or has less than two periods\n",
    "        '''\n",
    "        trend = stl.trend\n",
    "        seasonality = stl.seasonal\n",
    "        remainder = x_transformed - trend - seasonality\n",
    "\n",
    "    else:\n",
    "        # Nonseasonal data\n",
    "        trend = pd.Series(np.nan, index=x_transformed.index)\n",
    "        time_index = pd.Index([i for i in range(1,len(x_transformed)+1)])\n",
    "        # Python specific\n",
    "        bs = BSplines(time_index, df=[12, 10], degree=[3, 3])\n",
    "        cs = CyclicCubicSplines(time_index,df=[3,3])\n",
    "        alpha = np.array([218.338888])\n",
    "        gam = GLMGam(x_transformed, smoother=cs, alpha=alpha).fit()\n",
    "        #trend.loc[~x_transformed.isnull()] = gam.fittedvalues\n",
    "        \n",
    "        # R Code\n",
    "        fmla = Formula('x ~ s(tt)')\n",
    "        env = fmla.environment\n",
    "        env['tt'] = time_index\n",
    "        env['x'] = x_transformed\n",
    "        trend.loc[~x_transformed.isnull()] = rstats.fitted(rmgcv.gam(fmla))\n",
    "        seasonality = pd.Series(np.nan, index=x_transformed.index)\n",
    "        remainder = x_transformed - trend\n",
    "    \n",
    "    return_dct = {\n",
    "        'x': x_transformed,\n",
    "        'trend': trend,\n",
    "        'seasonality': seasonality,\n",
    "        'remainder': remainder,\n",
    "        'transform': transform,\n",
    "        'lambda': var_lambda,\n",
    "    }\n",
    "    \n",
    "    return return_dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Functions to map all the features onto a [0,1] scale\\n# f1 maps [0,infinity) to [0,1]\\nf1 <- function(x,a,b)\\n{\\n  eax <- exp(a*x)\\n  if (eax == Inf)\\n    f1eax <- 1\\n  else\\n    f1eax <- (eax-1)/(eax+b)\\n  return(f1eax)\\n}\\n\\n# f2 maps [0,1] onto [0,1]\\nf2 <- function(x,a,b)\\n{\\n  eax <- exp(a*x)\\n  ea <- exp(a)\\n  return((eax-1)/(eax+b)*(ea+b)/(ea-1))\\n}\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def f1_transformation(x, a, b):\n",
    "    eax = math.exp(a * x)\n",
    "    if eax == np.Inf:\n",
    "        f1_eax = 1\n",
    "    else:\n",
    "        f1_eax = (eax-1)/(eax+b)\n",
    "    return f1_eax\n",
    "\n",
    "def f2_transformation(x, a, b):\n",
    "    eax = math.exp(a*x)\n",
    "    ea = math.exp(a)\n",
    "    return((eax-1)/(eax+b)*(ea+b)/(ea-1))\n",
    "\n",
    "'''\n",
    "# Functions to map all the features onto a [0,1] scale\n",
    "# f1 maps [0,infinity) to [0,1]\n",
    "f1 <- function(x,a,b)\n",
    "{\n",
    "  eax <- exp(a*x)\n",
    "  if (eax == Inf)\n",
    "    f1eax <- 1\n",
    "  else\n",
    "    f1eax <- (eax-1)/(eax+b)\n",
    "  return(f1eax)\n",
    "}\n",
    "\n",
    "# f2 maps [0,1] onto [0,1]\n",
    "f2 <- function(x,a,b)\n",
    "{\n",
    "  eax <- exp(a*x)\n",
    "  ea <- exp(a)\n",
    "  return((eax-1)/(eax+b)*(ea+b)/(ea-1))\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Neural Network Test\n",
    "\"Nonlinear  time  series  models  have  been  used  extensively  in  recent  years  to  model complex dynamics not adequately represented use linear models ... Because of the special characteristic (behavior) of time series data, the traditional linear models cannot handle the  forecasting  well  compared  to  non-linear  models.  Therefore,  non-linearity  is  animportant characteristic of time series data to determine the selection of appropriate forecasting method.\n",
    "\n",
    "There are many approaches to test the nonlinearity in time series regression models. Nonparametric kernel test and neural network test are the two major models appeared in the literature. In the comparative studies between these two approaches, neural net-work has been reported with better reliability. In this research, we used Teraesvirta’s  neural  network  test  (Ter ̈aesvirta  et  al.,1993)  for  time  series  data  non-linearity characteristics identification and extraction. It has been widely accepted andreported that it can correctly model the nonlinear structure of the data (Rocca and Perna,2004). It is a test for neglected nonlinearity likely to have power against a range of al-ternatives based on neural network model (augmented single-hidden-layer feed forward neural network model). The test is based on a test function chosen as the activationsof ‘phantom’ hidden units.\"\n",
    "\n",
    "(4) (PDF) Characteristic-Based Clustering for Time Series Data. Available from: https://www.researchgate.net/publication/220451959_Characteristic-Based_Clustering_for_Time_Series_Data [accessed Jun 16 2020]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=0, lrate=0.500, error=6.350\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=1, lrate=0.500, error=5.531\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=2, lrate=0.500, error=5.221\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=3, lrate=0.500, error=4.951\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=4, lrate=0.500, error=4.519\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=5, lrate=0.500, error=4.173\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=6, lrate=0.500, error=3.835\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=7, lrate=0.500, error=3.506\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=8, lrate=0.500, error=3.192\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=9, lrate=0.500, error=2.898\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=10, lrate=0.500, error=2.626\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=11, lrate=0.500, error=2.377\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=12, lrate=0.500, error=2.153\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=13, lrate=0.500, error=1.953\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=14, lrate=0.500, error=1.774\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=15, lrate=0.500, error=1.614\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=16, lrate=0.500, error=1.472\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=17, lrate=0.500, error=1.346\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=18, lrate=0.500, error=1.233\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=19, lrate=0.500, error=1.132\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=20, lrate=0.500, error=1.042\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=21, lrate=0.500, error=0.961\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=22, lrate=0.500, error=0.887\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=23, lrate=0.500, error=0.821\n",
      "Inputs: [2.7810836, 2.550537003, 0]\n",
      "Inputs: [1.465489372, 2.362125076, 0]\n",
      "Inputs: [3.396561688, 4.400293529, 0]\n",
      "Inputs: [1.38807019, 1.850220317, 0]\n",
      "Inputs: [3.06407232, 3.005305973, 0]\n",
      "Inputs: [7.627531214, 2.759262235, 1]\n",
      "Inputs: [5.332441248, 2.088626775, 1]\n",
      "Inputs: [6.922596716, 1.77106367, 1]\n",
      "Inputs: [8.675418651, -0.242068655, 1]\n",
      "Inputs: [7.673756466, 3.508563011, 1]\n",
      ">epoch=24, lrate=0.500, error=0.761\n",
      "[{'weights': [-1.561955549575558, 2.025789218074887, 1.1500157356222], 'output': 0.027707969973765866, 'delta': -0.004723370639731328}, {'weights': [0.4335287646386403, -0.2732097727357278, 0.20935802381019797], 'output': 0.9205201273522332, 'delta': 0.0035498490232699275}]\n",
      "[{'weights': [2.9008101434736124, -0.5041753969983884, -1.0326867709467105], 'output': 0.1999081416467633, 'delta': -0.031974283037051646}, {'weights': [-2.8879811809547182, 1.1630795064775552, 0.5016495802228915], 'output': 0.8124263813512703, 'delta': 0.02858429802213756}]\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "from math import exp\n",
    "from random import seed\n",
    "from random import random\n",
    " \n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    " \n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    " \n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    " \n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    print('Inputs:', inputs)\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    " \n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    " \n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    " \n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    " \n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network_example(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    " \n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        #sum_error = 0\n",
    "        network_outputs = []\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            network_outputs.append(outputs)\n",
    "            #expected = [0 for i in range(n_outputs)]\n",
    "            #expected[row[-1]] = 1\n",
    "            #sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            #backward_propagate_error(network, expected)\n",
    "            #update_weights(network, row, l_rate)\n",
    "        #sum_output = sum(network_output)\n",
    "        #print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\n",
    "# Test training backprop algorithm\n",
    "seed(1)\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "[1.465489372,2.362125076,0],\n",
    "[3.396561688,4.400293529,0],\n",
    "[1.38807019,1.850220317,0],\n",
    "[3.06407232,3.005305973,0],\n",
    "[7.627531214,2.759262235,1],\n",
    "[5.332441248,2.088626775,1],\n",
    "[6.922596716,1.77106367,1],\n",
    "[8.675418651,-0.242068655,1],\n",
    "[7.673756466,3.508563011,1]]\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network_example(network, dataset, 0.5, 25, n_outputs)\n",
    "for layer in network:\n",
    "    print(layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in RPy2 and configure settings so that we can perform the Neural Network Test + FARIMA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import inv_boxcox\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "#from pypr.stattest.ljungbox import boxpierce\n",
    "def calculate_measures(x):\n",
    "    # Save ts version of our data for some of the below functions\n",
    "    rbase.set_seed(123) # reproducibility seed\n",
    "    x_ts_contiguous = r.ts(FloatVector(na_contiguous(x)))\n",
    "    #x = x_ts_contiguous\n",
    "    #print(x_ts_contiguous)\n",
    "    \n",
    "    # Now \"activate\" pandas2ri and numpy2ri\n",
    "    #pandas2ri.activate()\n",
    "    #numpy2ri.activate()\n",
    "    \n",
    "    N = len(x)\n",
    "    freq = find_freq_r(x_ts_contiguous)\n",
    "    fx = (math.exp((freq-1)/50)-1)/(1+math.exp((freq-1)/50))\n",
    "    \n",
    "    # Decomposition\n",
    "    decomp_x = decompose(x)\n",
    "    \n",
    "    # Adjust data\n",
    "    # Unfortunately it looks like frequency is calculated a different way in the decompose function\n",
    "    # Thus there are users for which this function is evaulated when 'seasonality' is null\n",
    "    # Going to add an extra check to make sure to not evaluate this if all the values are null\n",
    "    #print(decomp_x['seasonality'])\n",
    "    if freq > 1 and (not decomp_x['seasonality'].isnull().all()):\n",
    "        fit = decomp_x['trend'] + decomp_x['seasonality']\n",
    "    else:\n",
    "        # Nonseasonal data\n",
    "        fit = decomp_x['trend']\n",
    "    adj_x = decomp_x['x'] - fit + np.mean(decomp_x['trend'].dropna())\n",
    "    \n",
    "    # Backtransformation of adjusted data\n",
    "    if decomp_x['transform']:\n",
    "        # The below line of code doesn't work for some reason\n",
    "        #t_adj_x = inv_boxcox(adj_x.values, decomp_x['lambda'])\n",
    "        # Use actual formula instead (but do inverse because we're solving for x)\n",
    "        '''\n",
    "        The Box-Cox transform is given by:\n",
    "\n",
    "            y = (x**lmbda - 1) / lmbda,  for lmbda > 0\n",
    "                log(x),                  for lmbda = 0\n",
    "        '''\n",
    "        if decomp_x['lambda'] == 0:\n",
    "            # Assuming base of 10 (x = 10^y)\n",
    "            t_adj_x = 10 ** adj_x\n",
    "        else:\n",
    "            # x = ((y * lambda) + 1) ^ (1/lambda)\n",
    "            t_adj_x = ((adj_x * decomp_x['lambda']) + 1) ** (1/decomp_x['lambda'])\n",
    "    else:\n",
    "        t_adj_x = adj_x\n",
    "    \n",
    "    # Trend and seasonal measures\n",
    "    v_adj = np.var(adj_x.dropna())\n",
    "    threshold = 0.00000000001\n",
    "    if(freq > 1):\n",
    "        detrend = decomp_x['x'] - decomp_x['trend']\n",
    "        deseason = decomp_x['x'] - decomp_x['seasonality']\n",
    "        \n",
    "        if np.var(deseason.dropna()) < threshold:\n",
    "            trend = 0\n",
    "        else:\n",
    "            trend = max(0,min(1,1-(v_adj/np.var(deseason.dropna()))))\n",
    "        if np.var(detrend.dropna()) < threshold:\n",
    "            seasonality = 0\n",
    "        else:\n",
    "            seasonality = max(0,min(1,1-(v_adj/np.var(detrend.dropna()))))\n",
    "    else:\n",
    "        # Nonseasonal data\n",
    "        if np.var(decomp_x['x'].dropna()) < threshold:\n",
    "            trend = 0\n",
    "        else:\n",
    "            trend = max(0,min(1,1-(v_adj/np.var(decomp_x['x'].dropna()))))\n",
    "        seasonality = 0\n",
    "    \n",
    "    measures = [fx,trend,seasonality]\n",
    "    \n",
    "    # Measures on original data\n",
    "    xbar = np.mean(x.dropna())\n",
    "    std = np.std(x.dropna())\n",
    "    \n",
    "    # Serial correlation (make sure box pierce statistic is returned as well)\n",
    "    #bp = boxpierce(x, lags=max_lag)\n",
    "    #Had to fix stattest module in pypr package via: https://gist.github.com/betterxys/1def38e1fcbb7f3b2dab2393bcea52f0\n",
    "    max_lag = 10\n",
    "    lbvalue, pvalue, bpvalue, bppvalue = acorr_ljungbox(x, lags=max_lag, boxpierce=True)\n",
    "    # The above returns values for each lag, so just grab the final value\n",
    "    Q = bpvalue[-1] / (N*max_lag)\n",
    "    fQ = f2_transformation(Q,7.53,0.103)\n",
    "    \n",
    "    # Nonlinearity (THIS REQUIRES THE TIMESERIES OBJECT VERSION OF OUR DATA)\n",
    "\n",
    "    non_linear_test = rtseries.terasvirta_test_ts(x_ts_contiguous,type = \"Chisq\")\n",
    "    #non_linear_test = rtseries.terasvirta_test_default(y=x_contiguous,x=x_contiguous.index.dayofyear,type = \"Chisq\")\n",
    "    '''\n",
    "    x = r.ts(FloatVector(na_contiguous(analysis_data_by_user_id[100613640])))\n",
    "    non_linear_test = rtseries.terasvirta_test_ts(x,type = \"Chisq\")\n",
    "    print(non_linear_test.names)\n",
    "    [1] \"statistic\" \"parameter\" \"p.value\"   \"method\"    \"data.name\" \"arguments\"\n",
    "    '''\n",
    "    p = list(non_linear_test[0])[0]\n",
    "    fp = f1_transformation(p,0.069,2.304)\n",
    "    \n",
    "    # Skewness\n",
    "    skew = abs(np.mean((x.dropna()-xbar) ** 3)/std ** 3)\n",
    "    fs = f1_transformation(skew,1.510,5.993)\n",
    "    \n",
    "    # Kurtosis\n",
    "    kurtosis = np.mean((x.dropna()-xbar) ** 4)/std ** 4\n",
    "    fk = f1_transformation(kurtosis,2.273,11567)\n",
    "    \n",
    "    # Hurst=d+0.5 where d is fractional difference\n",
    "    hurst = rfracdiff.fracdiff(x_ts_contiguous,0,0)\n",
    "    '''\n",
    "    x = r.ts(FloatVector(na_contiguous(analysis_data_by_user_id[100613640])))\n",
    "    hurst = rfracdiff.fracdiff(x_ts_contiguous,0,0)\n",
    "    print(hurst.names)\n",
    "     [1] \"log.likelihood\"  \"n\"               \"msg\"             \"d\"              \n",
    "     [5] \"ar\"              \"ma\"              \"covariance.dpq\"  \"fnormMin\"       \n",
    "     [9] \"sigma\"           \"stderror.dpq\"    \"correlation.dpq\" \"h\"              \n",
    "    [13] \"d.tol\"           \"M\"               \"hessian.dpq\"     \"length.w\"       \n",
    "    [17] \"residuals\"       \"fitted\"          \"call\"     \n",
    "    '''\n",
    "    # Grab the fourth value in the hurst variable\n",
    "    H = list(hurst[3])[0] + 0.5\n",
    "    \n",
    "    # Lyapunov Exponent\n",
    "    if freq > (N-10):\n",
    "        # There is insufficient data, declare this variable as none\n",
    "        fLyap = None\n",
    "    else:\n",
    "        Ly = np.zeros(N-freq)\n",
    "        for i in range(0,(N-freq)):\n",
    "            diffs = abs(x.iloc[i] - x)\n",
    "            date_idx = diffs.sort_values().index\n",
    "            int_idx = pd.Index([diffs.index.get_loc(date) for date in date_idx])\n",
    "            idx = int_idx[int_idx < (N-freq)]\n",
    "            j = idx[1]\n",
    "            try:\n",
    "                Ly[i] = math.log(abs((x.iloc[i+freq] - x.iloc[j+freq])/(x.iloc[i]-x.iloc[j]))) / freq\n",
    "            except ValueError: # domain error, means log(0) was taken\n",
    "                Ly[i] = 0\n",
    "            if(np.isnan(Ly[i]) or (Ly[i] == np.Inf) or (Ly[i] == -np.Inf)):\n",
    "                Ly[i] = np.nan\n",
    "        Lyap = np.mean(Ly[~np.isnan(Ly)])\n",
    "        fLyap = math.exp(Lyap) / (1+math.exp(Lyap))\n",
    "    \n",
    "    measures = measures + [fQ,fp,fs,fk,H,fLyap]\n",
    "    \n",
    "    # Measures on adjusted data\n",
    "    xbar = np.mean(t_adj_x.dropna())\n",
    "    std = np.std(t_adj_x.dropna())\n",
    "\n",
    "    # Serial correlation (make sure box pierce statistic is returned as well)\n",
    "    #bp = boxpierce(adj_x, lags=max_lag)\n",
    "    max_lag = 10\n",
    "    lbvalue, pvalue, bpvalue, bppvalue = acorr_ljungbox(na_contiguous(adj_x), lags=max_lag, boxpierce=True)\n",
    "    # The above returns values for each lag, so just grab the final value\n",
    "    Q = bpvalue[-1] / (N*max_lag)\n",
    "    fQ = f2_transformation(Q,7.53,0.103)\n",
    "\n",
    "    # Nonlinearity (add try/except block to capture USER IDs where this doesn't work)\n",
    "    # (THIS REQUIRES THE TIMESERIES OBJECT VERSION OF OUR DATA)\n",
    "    adj_x_contiguous = r.ts(FloatVector(na_contiguous(adj_x)))\n",
    "    non_linear_test = rtseries.terasvirta_test_ts(adj_x_contiguous,type = \"Chisq\")\n",
    "    #non_linear_test = rtseries.terasvirta_test_default(y=adj_x_contiguous,x=adj_x_contiguous.index.dayofyear,type = \"Chisq\")\n",
    "    '''\n",
    "    x = r.ts(FloatVector(na_contiguous(analysis_data_by_user_id[100613640])))\n",
    "    non_linear_test = rtseries.terasvirta_test_ts(x,type = \"Chisq\")\n",
    "    print(non_linear_test.names)\n",
    "    [1] \"statistic\" \"parameter\" \"p.value\"   \"method\"    \"data.name\" \"arguments\"\n",
    "    '''\n",
    "    # Grab first element\n",
    "    p = list(non_linear_test[0])[0]\n",
    "    fp = f1_transformation(p,0.069,2.304)\n",
    "    \n",
    "    # Skewness\n",
    "    skew = abs(np.mean((t_adj_x.dropna() - xbar) ** 3)/(std ** 3))\n",
    "    fs = f1_transformation(skew,1.510,5.993)\n",
    "\n",
    "    # Kurtosis\n",
    "    kurtosis = np.mean((t_adj_x.dropna() - xbar) ** 4)/(std ** 4)\n",
    "    fk = f1_transformation(kurtosis,2.273,11567)\n",
    "    \n",
    "    measures_list = measures + [fQ,fp,fs,fk]\n",
    "\n",
    "    return measures_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data\n",
    "\n",
    "The code here was based on input data that looked like the following:\n",
    "```\n",
    "   DATE_COLUMN  ID_COL  COL_1   COL_2\tCOL_3\tCOL_4\tCOL_5\tCOL_6\n",
    "0\t2020-01-01\t  1    71.068\t71.880\t48.989\t91.525\t268\t25\t1.94\n",
    "1\t2020-01-02\t  1    67.620\t58.387\t60.222\t93.487\t323\t54\t5.65\n",
    "2\t2020-01-03\t  2    79.221\t82.174\t66.070\t86.466\t476\t103\t6.59\n",
    "3\t2020-01-04\t  2    74.678\t80.550\t66.839\t70.774\t464\t68\t10.56\n",
    "...\n",
    "```\n",
    "\n",
    "The `ID_COL` column was an identifying value distinguishing between different users on a platform. The below code separates and groups data by this column and then does the feature extraction on each grouping; if there is no identifying value then that step can be skipped and run on a dataset consisting of a date column and a numerical column.\n",
    "\n",
    "Ideally there is enough data where trends take shape and meaningful features could actually be extracted (ideally more than a year).\n",
    "\n",
    "Also, there isn't a specific method for addressing missing data in the paper; that will need to be chosen beforehand.\n",
    "\n",
    "Example code is shown below of what the whole thing would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Declare variables that we want to analyze \\n#analysis_df = pd.read_csv(...)\\nanalysis_col = \\'COL_1\\'\\ndate_col = \\'DATE_COLUMN\\'\\nanalysis_columns = [\\'ID_COL\\', date_col, analysis_col]\\nanalysis_data = analysis_df[analysis_columns]\\n# Group data by ID\\nanalysis_data_by_id = {}\\nfor id in analysis_data[\\'ID_COL\\'].unique():\\n    subset_df = analysis_data.loc[analysis_data[\\'ID_COL\\'].isin([id])]\\n    sorted_subset_df = subset_df.sort_values(by=[\\'ID_COL\\',date_col])\\n    analysis_data_by_id[id] = sorted_subset_df[[date_col,analysis_col]].set_index(date_col)[analysis_col]\\n    \\nmeasures_dct = {}\\nfor id,series in analysis_data_by_user_id.items():\\n    print(\\'Now calculating measures for user \\',id)\\n    measures_dct[id] = calculate_measures(series)\\n    \\nmeasures_df = pd.DataFrame.from_dict(measures_dct,orient=\\'index\\',columns=[\"frequency\", \"trend\",\"seasonal\", \"autocorrelation\",\"non-linear\",\"skewness\",\"kurtosis\",\"Hurst\",\"Lyapunov\",\"dc autocorrelation\",\"dc non-linear\",\"dc skewness\",\"dc kurtosis\"])\\ndisplay(measures_df)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Declare variables that we want to analyze \n",
    "#analysis_df = pd.read_csv(...)\n",
    "analysis_col = 'COL_1'\n",
    "date_col = 'DATE_COLUMN'\n",
    "analysis_columns = ['ID_COL', date_col, analysis_col]\n",
    "analysis_data = analysis_df[analysis_columns]\n",
    "# Group data by ID\n",
    "analysis_data_by_id = {}\n",
    "for id in analysis_data['ID_COL'].unique():\n",
    "    subset_df = analysis_data.loc[analysis_data['ID_COL'].isin([id])]\n",
    "    sorted_subset_df = subset_df.sort_values(by=['ID_COL',date_col])\n",
    "    analysis_data_by_id[id] = sorted_subset_df[[date_col,analysis_col]].set_index(date_col)[analysis_col]\n",
    "    \n",
    "measures_dct = {}\n",
    "for id,series in analysis_data_by_user_id.items():\n",
    "    print('Now calculating measures for user ',id)\n",
    "    measures_dct[id] = calculate_measures(series)\n",
    "    \n",
    "measures_df = pd.DataFrame.from_dict(measures_dct,orient='index',columns=[\"frequency\", \"trend\",\"seasonal\", \"autocorrelation\",\"non-linear\",\"skewness\",\"kurtosis\",\"Hurst\",\"Lyapunov\",\"dc autocorrelation\",\"dc non-linear\",\"dc skewness\",\"dc kurtosis\"])\n",
    "display(measures_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example output data\n",
    "\n",
    "```\n",
    "id_col frequency\ttrend\tseasonal\tautocorrelation\tnon-linear\tskewness\tkurtosis\tHurst\tLyapunov\tdc autocorrelation\tdc non-linear\tdc skewness\tdc kurtosis\n",
    "1\t0.059928\t0.124919\t0.243887\t0.109589\t0.028767\t0.067714\t0.046443\t0.500046\t0.618593\t0.206500\t0.044546\t0.060988\t0.237828\n",
    "2\t0.000000\t0.267712\t0.000000\t0.240373\t0.004038\t0.140565\t0.040838\t0.729902\t0.982717\t0.230324\t0.026835\t0.142644\t0.108635\n",
    "3\t0.069886\t0.118976\t0.456264\t0.298858\t0.053476\t0.015268\t0.037035\t0.500046\t0.619930\t0.165401\t0.002541\t0.038293\t0.087527\n",
    "4\t0.000000\t0.166727\t0.000000\t0.106591\t0.030656\t0.281176\t0.284982\t0.666537\t0.983842\t0.139930\t0.025257\t0.431716\t0.988605\n",
    "5\t0.000000\t0.272094\t0.000000\t0.096659\t0.046871\t0.259884\t0.222697\t0.555352\t0.985034\t0.176478\t0.006834\t0.174641\t0.065182\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
