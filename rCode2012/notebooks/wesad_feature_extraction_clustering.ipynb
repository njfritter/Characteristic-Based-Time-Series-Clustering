{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wearable Stress and Affect Detection (WESAD) Feature Extraction & Clustering\n",
    "\n",
    "[UCI Link](https://archive.ics.uci.edu/ml/datasets/WESAD+%28Wearable+Stress+and+Affect+Detection%29#)\n",
    "\n",
    "''' Philip Schmidt, Attila Reiss, Robert Duerichen, Claus Marberger and Kristof Van Laerhoven. 2018. Introducing WESAD, a multimodal dataset for Wearable Stress and Affect Detection. In 2018 International Conference on Multimodal Interaction (ICMI '18), October 16-20, 2018, Boulder, CO, USA. ACM, New York, NY, USA, 9 pages. '''\n",
    "\n",
    "This dataset is part of the UCI ML Data repository and contains high granularity data (700 Hz) of 15 test subjects from chest worn sensors (RespiBAN) in the form of:\n",
    "\n",
    "- Electrocardiography (ECG)\n",
    "- Electrodermal Activity (EDA)\n",
    "- Electromyography (EMG)\n",
    "- Body Temp (Temp)\n",
    "- Accelorometer (ACC)\n",
    "- Respiration % (Resp)\n",
    "\n",
    "Contains data at lower granularity from wrist worn (non dominant) Empatica device in the form of:\n",
    "\n",
    "- Accelorometer (ACC)\n",
    "- Blood Volume Pulse (BVP)\n",
    "  - This feature wasn't actually explained in the connected README for the dataset, so using the information [here](https://biosignalsplux.com/products/sensors/blood-volume-pulse.html) which is the same source as for the other data\n",
    "- Electrodermal Activity (EDA)\n",
    "- Body Temp (Temp)\n",
    "\n",
    "Wearable data generation has exploded in recent years, and with it the analysis of it. Time series data can yield very interesting insights and can paint a picture of people's health that they would not be able to see themselves.\n",
    "\n",
    "Exploratory analysis was done in separate notebooks; this notebook will specifically look at feature extraction from the original dense data then clustering based on those features. Specifically, this notebook will leverage the R code found in this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The features (Based on original paper [here](https://www.researchgate.net/publication/220451959_Characteristic-Based_Clustering_for_Time_Series_Data))\n",
    "\n",
    "The following features will be extracted from each time series as inputs for clustering:\n",
    "\n",
    "**On Original Data:**\n",
    "- Frequency\n",
    "- Trend\n",
    "- Seasonality\n",
    "- Autocorrelation\n",
    "- Non-linearity\n",
    "- Skewness\n",
    "- Kurtosis\n",
    "- Hurst\n",
    "- Lyapunov\n",
    "\n",
    "**On Decomposed Data (with trend & seasonality removed):**\n",
    "- Decomposed Autocorrelation\n",
    "- Decomposed Non-linearity\n",
    "- Decomposed Skewness\n",
    "- Decomposed kurtosis\n",
    "\n",
    "More on each of these features is contained in the explanation markdown file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Original R Code](https://www.r-bloggers.com/measuring-time-series-characteristics/)\n",
    "\n",
    "The above link is to the R code that this code will be based off of.\n",
    "\n",
    "After seeing the above link, I tried my hand at turning this code into Python. I was able to get slightly more than half of the above features equivalent to the results from the R code. Some of the features simply wouldn't match up, and others simply could not be computed in Python due to lack of supporting packages/documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In R as well (since I can't get it to work in Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires some manual work before these code blocks will work\n",
    "# 1. Set R_HOME variable (can be found by typing \".Library\" into R Studio)\n",
    "# 2. The output from the above is the \"library\" subdirectory of the R_HOME directory established by R Studio\n",
    "# 3. Take the output minus \"/library\" at the end, and save this to R_HOME in a terminal\n",
    "# 4. Run pip3 install rpy2 (connects to R_HOME at install)\n",
    "# Will make an init.sh file to run all the necessary steps for this + other manual setup steps\n",
    "\n",
    "# Sources: https://stackoverflow.com/questions/17573988/r-home-error-with-rpy2\n",
    "# https://stackoverflow.com/questions/47585718/rpy2-installed-but-wont-run-packages\n",
    "# https://stackoverflow.com/questions/24880493/how-to-find-out-r-library-location-in-mac-osx/24880594\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import r, pandas2ri, numpy2ri, Formula\n",
    "from rpy2.robjects.vectors import IntVector,FloatVector\n",
    "# Load necessary R packages\n",
    "rtseries = importr('tseries')\n",
    "rbase = importr('base')\n",
    "rstats = importr('stats')\n",
    "rfracdiff=importr('fracdiff')\n",
    "rutils=importr('utils')\n",
    "rmgcv=importr('mgcv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Activating\" pandas2ri and numpy2ri with .activate() method\n",
    "\n",
    "Typically, the above libraries are \"activated\" before use to make working with the python dataframes easier. For some reason, running the above commands blocks me from being able to create R time series objects. Trying to create time series objects just leads to numpy arrays being created(used the following [resource](https://www.r-bloggers.com/using-r-in-python-for-statistical-learning-data-science-2/) to help).\n",
    "\n",
    "Example output when pandas2ri and numpy2ri are NOT activated:\n",
    "```\n",
    "rbase.set_seed(123) # reproducibility seed\n",
    "x = r.ts(r.rnorm(n=10)) # simulate the time series\n",
    "print(x)\n",
    "\n",
    "Time Series:\n",
    "Start = 1 \n",
    "End = 10 \n",
    "Frequency = 1 \n",
    " [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n",
    " [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n",
    "```\n",
    "\n",
    "This is the format we want; otherwise certain R command will not work because they explicitly want a time series object (seen above) as input.\n",
    "\n",
    "Example output when pandas2ri and numpy2ri ARE activated:\n",
    "\n",
    "```\n",
    "pandas2ri.activate()\n",
    "numpy2ri.activate()\n",
    "rbase.set_seed(123) # reproducibility seed\n",
    "x = r.ts(r.rnorm(n=10)) # simulate the time series\n",
    "print(x)\n",
    "\n",
    "[-0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n",
    "  0.46091621 -1.26506123 -0.68685285 -0.44566197]\n",
    "```\n",
    "\n",
    "I have not actually run the code myself because once the \"activate\" methods are called, I have not found a way to be able to create time series objects without restarting the whole notebook/shell. You are welcome to try out the commands for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get frequency\n",
    "\n",
    "The R code has three functions, one of which is to automatically determine the frequency of the data by using AR spectrum modeling on the data as seen below:\n",
    "\n",
    "\n",
    "```\n",
    "# Function to find the frequency of the time series data inputted\n",
    "find.freq <- function(x)\n",
    "{\n",
    "  n <- length(x)\n",
    "  spec <- spec.ar(c(na.contiguous(x)),plot=FALSE)\n",
    "  if(max(spec$spec)>10) # Arbitrary threshold chosen by trial and error.\n",
    "  {\n",
    "    period <- round(1/spec$freq[which.max(spec$spec)])\n",
    "    if(period==Inf) # Find next local maximum\n",
    "    {\n",
    "      j <- which(diff(spec$spec)>0)\n",
    "      if(length(j)>0)\n",
    "      {\n",
    "        nextmax <- j[1] + which.max(spec$spec[j[1]:500])\n",
    "        if(nextmax <= length(spec$freq))\n",
    "          period <- round(1/spec$freq[nextmax])\n",
    "        else\n",
    "          period <- 1\n",
    "      }\n",
    "      else\n",
    "        period <- 1\n",
    "    }\n",
    "  }\n",
    "  else\n",
    "    period <- 1\n",
    "\n",
    "  return(period)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Version (doesn't match up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First find frequency\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import periodogram\n",
    "def find_freq(x):\n",
    "    # Use an iterative function to automagically determine the frequency of the time series data\n",
    "    # Takes in a single column of a pandas DataFrame as a univariate time series\n",
    "    \n",
    "    n = len(x)\n",
    "    # Now estimate the spectral density of the time series via AR fit\n",
    "    # Two ways: numpy fft method or scipy signal method\n",
    "    # Method #1: numpy fft method (https://stackoverflow.com/questions/15382076/plotting-power-spectrum-in-python)\n",
    "    '''\n",
    "    pow_np = np.abs(np.fft.fft(x))**2\n",
    "    time_step = 1 / n\n",
    "    freqs = np.fft.fftfreq(n, time_step)\n",
    "    idx = np.argsort(freqs)\n",
    "    plt.plot(freqs[idx], ps[idx])\n",
    "    '''\n",
    "    \n",
    "    # Method #2: scipy signal method via density or spectrum (takes an optional second frequency parameter)\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.periodogram.html\n",
    "    #f, pow_scipy = signal.periodogram(x,scaling='density')\n",
    "    f, pow_scipy = periodogram(x,scaling='spectrum')\n",
    "    \n",
    "    # Iterate through frequencies \n",
    "    freq = f\n",
    "    power = pow_scipy\n",
    "    if max(power) > 10: # Arbitrary threshold chosen by trial and error.\n",
    "        # The power might be a huge number way of out the index bounds, so pick max index if so\n",
    "        power_idx = min(int(round(max(power))),(len(power)-1))\n",
    "        freq_idx = min(int(round(power[power_idx])),len(freq)-1)\n",
    "        period = 1/freq[freq_idx]\n",
    "        # If period is infinity, find next local maximum\n",
    "        if period == np.Inf:\n",
    "            j = pd.Series(power).diff() > 0\n",
    "            if len(j) > 0:\n",
    "                nextmax = j[1] + power[int(round(max(power[1:])))]\n",
    "                if (nextmax <= len(freq)):\n",
    "                    period = int(round(1/freq[int(round(nextmax))]))\n",
    "                else:\n",
    "                    period = 1\n",
    "            else:\n",
    "                period = 1\n",
    "        else:\n",
    "            period = int(round(period))\n",
    "    else:\n",
    "        period = 1\n",
    "    \n",
    "    return period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R Code in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_freq_r(x):\n",
    "    # Same as above function, but using the R code directly via rpy2\n",
    "    n = len(x)\n",
    "    #spec = rstats.spec_ar(r.ts(na_contiguous(x)),plot=False)\n",
    "    spec = rstats.spec_ar(x,plot=False)\n",
    "    # This returns a \"ListVector\" with the following items, which can be accessed via list indexing:\n",
    "    '''\n",
    "    Example output\n",
    "    x = r.ts(r.rnorm(n=30)) # simulate the time series\n",
    "    spec = rstats.spec_ar(x,plot=False)\n",
    "    print(spec.names)\n",
    "    \n",
    "    [1] \"freq\"   \"spec\"   \"coh\"    \"phase\"  \"n.used\" \"series\" \"method\"\n",
    "    '''\n",
    "    spec_vals = spec[1]\n",
    "    spec_freq = spec[0]\n",
    "\n",
    "    if max(spec_vals) > 10:\n",
    "        #period <- round(1/spec$freq[which.max(spec$spec)])\n",
    "        denom = spec_freq[list(rbase.which_max(spec_vals))[0] - 1]\n",
    "        if denom != 0:\n",
    "            period = round(1/denom)\n",
    "        else: # Means we end up with infinity as a result, so evaluate additional code block\n",
    "            series = pd.Series(rbase.diff(spec_vals))\n",
    "            j = series[series > 0].reset_index(drop=True)\n",
    "            if len(j) > 0:\n",
    "                #nextmax <- j[1] + which.max(spec$spec[j[1]:500])\n",
    "                nextmax = j[0] + rbase.which_max(spec_vals[int(j[0]):500])\n",
    "                if nextmax.item() - 1 <= len(spec_freq):\n",
    "                    denom = spec_freq[round(nextmax.item() - 1)]\n",
    "                    if denom != 0:\n",
    "                        period = round(1/denom)\n",
    "                    else:\n",
    "                        period = 1\n",
    "                else:\n",
    "                    period = 1\n",
    "            else:\n",
    "                period = 1\n",
    "    else:\n",
    "        period = 1\n",
    "    \n",
    "    return int(period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose function\n",
    "\n",
    "The second of the three functions in the R code takes a time series object and extracts the trend and seasonality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "#from statsmodels.gam.api import GLMGam, BSplines\n",
    "from statsmodels.gam.generalized_additive_model import GLMGam\n",
    "from statsmodels.gam.smooth_basis import BSplines,CyclicCubicSplines\n",
    "import statsmodels.api as sm\n",
    "#from statsmodels.gam.generalized_additive_model import GLMGam\n",
    "\n",
    "def na_contiguous(x):\n",
    "    # Recreate na.contiguous function in R since this is used frequently\n",
    "    # This takes a series object with a time index and finds the longest consecutive stretch of non-missing values\n",
    "    # https://stackoverflow.com/questions/41494444/pandas-find-longest-stretch-without-nan-values\n",
    "    # And then return the shortened dataframe with all non-null values\n",
    "    values = x.values \n",
    "    mask = np.concatenate(( [True], np.isnan(values), [True] ))  # Mask\n",
    "    start_stop = np.flatnonzero(mask[1:] != mask[:-1]).reshape(-1,2)   # Start-stop limits\n",
    "    start,stop = start_stop[(start_stop[:,1] - start_stop[:,0]).argmax()]  # Get max interval, interval limits\n",
    "    contiguous = x.iloc[start:stop]\n",
    "    return contiguous\n",
    "\n",
    "def decompose(x, transform = True):\n",
    "    # Decompose data into trend, seasonality and randomness\n",
    "    # Accepts a pandas series object with a datetime index\n",
    "    if (transform and min(x.dropna()) >= 0):\n",
    "        # Transforms data and finds the lambda that maximizes the log likelihood \n",
    "        # R version has above method and method that minimizes the coefficient of variation (\"guerrero\")\n",
    "        x_transformed, var_lambda = boxcox(na_contiguous(x),lmbda = None)\n",
    "        x_transformed = pd.Series(x_transformed,index=na_contiguous(x).index)\n",
    "    \n",
    "    else:\n",
    "        x_transformed = x\n",
    "        var_lambda = np.nan\n",
    "        transform = False\n",
    "    \n",
    "    # Seasonal data \n",
    "    # In R code, we find the number of samples per unit time below (should be 1 every time)\n",
    "    # Tried implementing this in Python, but just went with the R code instead\n",
    "    # This is supposed to be \"> 1\" but all data results in a frequency of 1\n",
    "    # All frequency results in R equal 4, meaning this code block gets evaluated every time in R\n",
    "    # So this code block should always be evaluated as well\n",
    "    freq = rstats.frequency(r.ts(FloatVector(x_transformed)))\n",
    "    if int(list(freq)[0]) == 1:\n",
    "        # Decompose\n",
    "        stl = sm.tsa.seasonal_decompose(na_contiguous(x_transformed),period=1)\n",
    "        #stl = rstats.stl(na_contiguous(x_transformed),s_window='periodic')\n",
    "        # When I try to use above function, I get this:\n",
    "        '''\n",
    "        R[write to console]: Error in (function (x, s.window, s.degree = 0, t.window = NULL, t.degree = 1,  : \n",
    "  series is not periodic or has less than two periods\n",
    "        '''\n",
    "        trend = stl.trend\n",
    "        seasonality = stl.seasonal\n",
    "        remainder = x_transformed - trend - seasonality\n",
    "\n",
    "    else:\n",
    "        # Nonseasonal data\n",
    "        trend = pd.Series(np.nan, index=x_transformed.index)\n",
    "        time_index = pd.Index([i for i in range(1,len(x_transformed)+1)])\n",
    "        # Python specific\n",
    "        bs = BSplines(time_index, df=[12, 10], degree=[3, 3])\n",
    "        cs = CyclicCubicSplines(time_index,df=[3,3])\n",
    "        alpha = np.array([218.338888])\n",
    "        gam = GLMGam(x_transformed, smoother=cs, alpha=alpha).fit()\n",
    "        #trend.loc[~x_transformed.isnull()] = gam.fittedvalues\n",
    "        \n",
    "        # R Code\n",
    "        fmla = Formula('x ~ s(tt)')\n",
    "        env = fmla.environment\n",
    "        env['tt'] = time_index\n",
    "        env['x'] = x_transformed\n",
    "        trend.loc[~x_transformed.isnull()] = rstats.fitted(rmgcv.gam(fmla))\n",
    "        seasonality = pd.Series(np.nan, index=x_transformed.index)\n",
    "        remainder = x_transformed - trend\n",
    "    \n",
    "    return_dct = {\n",
    "        'x': x_transformed,\n",
    "        'trend': trend,\n",
    "        'seasonality': seasonality,\n",
    "        'remainder': remainder,\n",
    "        'transform': transform,\n",
    "        'lambda': var_lambda,\n",
    "    }\n",
    "    \n",
    "    return return_dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation (standardization) functions\n",
    "\n",
    "All of the features are standarized according to one of two of the following transformations:\n",
    "\n",
    "```\n",
    "# Functions to map all the features onto a [0,1] scale\n",
    "# f1 maps [0,infinity) to [0,1]\n",
    "f1 <- function(x,a,b)\n",
    "{\n",
    "  eax <- exp(a*x)\n",
    "  if (eax == Inf)\n",
    "    f1eax <- 1\n",
    "  else\n",
    "    f1eax <- (eax-1)/(eax+b)\n",
    "  return(f1eax)\n",
    "}\n",
    "\n",
    "# f2 maps [0,1] onto [0,1]\n",
    "f2 <- function(x,a,b)\n",
    "{\n",
    "  eax <- exp(a*x)\n",
    "  ea <- exp(a)\n",
    "  return((eax-1)/(eax+b)*(ea+b)/(ea-1))\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def f1_transformation(x, a, b):\n",
    "    eax = math.exp(a * x)\n",
    "    if eax == np.Inf:\n",
    "        f1_eax = 1\n",
    "    else:\n",
    "        f1_eax = (eax-1)/(eax+b)\n",
    "    return f1_eax\n",
    "\n",
    "def f2_transformation(x, a, b):\n",
    "    eax = math.exp(a*x)\n",
    "    ea = math.exp(a)\n",
    "    return((eax-1)/(eax+b)*(ea+b)/(ea-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Calculations\n",
    "\n",
    "Using the various functions above, we calculate the final measures as seen in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import inv_boxcox\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from pypr.stattest.ljungbox import boxpierce\n",
    "def calculate_measures(x):\n",
    "    # Save ts version of our data for some of the below functions\n",
    "    rbase.set_seed(123) # reproducibility seed\n",
    "    x_ts_contiguous = r.ts(FloatVector(na_contiguous(x)))\n",
    "    \n",
    "    N = len(x)\n",
    "    freq = find_freq_r(x_ts_contiguous)\n",
    "    print('Frequency calculated')\n",
    "    fx = (math.exp((freq-1)/50)-1)/(1+math.exp((freq-1)/50))\n",
    "    \n",
    "    # Decomposition\n",
    "    decomp_x = decompose(x)\n",
    "    print('Decomposition complete')\n",
    "    \n",
    "    # Adjust data\n",
    "    # Unfortunately it looks like frequency is calculated a different way in the decompose function\n",
    "    # Thus there are users for which this function is evaulated when 'seasonality' is null\n",
    "    # Going to add an extra check to make sure to not evaluate this if all the values are null\n",
    "    if freq > 1 and (not decomp_x['seasonality'].isnull().all()):\n",
    "        fit = decomp_x['trend'] + decomp_x['seasonality']\n",
    "    else:\n",
    "        # Nonseasonal data\n",
    "        fit = decomp_x['trend']\n",
    "    adj_x = decomp_x['x'] - fit + np.mean(decomp_x['trend'].dropna())\n",
    "    print('Adj x complete')\n",
    "    \n",
    "    # Backtransformation of adjusted data\n",
    "    if decomp_x['transform']:\n",
    "        # The below line of code doesn't work for some reason\n",
    "        #t_adj_x = inv_boxcox(adj_x.values, decomp_x['lambda'])\n",
    "        # Use actual formula instead (but do inverse because we're solving for x)\n",
    "        '''\n",
    "        The Box-Cox transform is given by:\n",
    "\n",
    "            y = (x**lmbda - 1) / lmbda,  for lmbda > 0\n",
    "                log(x),                  for lmbda = 0\n",
    "        '''\n",
    "        if decomp_x['lambda'] == 0:\n",
    "            # Assuming base of 10 (x = 10^y)\n",
    "            t_adj_x = 10 ** adj_x\n",
    "        else:\n",
    "            # x = ((y * lambda) + 1) ^ (1/lambda)\n",
    "            t_adj_x = ((adj_x * decomp_x['lambda']) + 1) ** (1/decomp_x['lambda'])\n",
    "    else:\n",
    "        t_adj_x = adj_x\n",
    "    \n",
    "    print('Calculating trend/seasonal measures')\n",
    "    # Trend and seasonal measures\n",
    "    v_adj = np.var(adj_x.dropna())\n",
    "    threshold = 0.00000000001\n",
    "    if(freq > 1):\n",
    "        detrend = decomp_x['x'] - decomp_x['trend']\n",
    "        deseason = decomp_x['x'] - decomp_x['seasonality']\n",
    "        \n",
    "        if np.var(deseason.dropna()) < threshold:\n",
    "            trend = 0\n",
    "        else:\n",
    "            trend = max(0,min(1,1-(v_adj/np.var(deseason.dropna()))))\n",
    "        if np.var(detrend.dropna()) < threshold:\n",
    "            seasonality = 0\n",
    "        else:\n",
    "            seasonality = max(0,min(1,1-(v_adj/np.var(detrend.dropna()))))\n",
    "    else:\n",
    "        # Nonseasonal data\n",
    "        if np.var(decomp_x['x'].dropna()) < threshold:\n",
    "            trend = 0\n",
    "        else:\n",
    "            trend = max(0,min(1,1-(v_adj/np.var(decomp_x['x'].dropna()))))\n",
    "        seasonality = 0\n",
    "    \n",
    "    measures = [fx,trend,seasonality]\n",
    "    \n",
    "    # Measures on original data\n",
    "    xbar = np.mean(x.dropna())\n",
    "    std = np.std(x.dropna())\n",
    "    print('Calculating serial correlation')\n",
    "    ### THIS IS WHERE THE ISSUE IS ###\n",
    "    \n",
    "    # Serial correlation (method 1)\n",
    "    #Had to fix stattest module in pypr package via: https://gist.github.com/betterxys/1def38e1fcbb7f3b2dab2393bcea52f0\n",
    "    max_lag = 10\n",
    "    bp = boxpierce(x, lags=max_lag)\n",
    "    Q = bp / (N*max_lag)\n",
    "    fQ = f2_transformation(Q,7.53,0.103)\n",
    "\n",
    "    # Serial correlation (make sure box pierce statistic is returned as well)\n",
    "    # Method 2, but ends up never finishing :(\n",
    "    #lbvalue, pvalue, bpvalue, bppvalue = acorr_ljungbox(x, lags=max_lag, boxpierce=True)\n",
    "    # The above returns values for each lag, so just grab the final value\n",
    "    #Q = bpvalue[-1] / (N*max_lag)\n",
    "    #fQ = f2_transformation(Q,7.53,0.103)\n",
    "    \n",
    "    # Nonlinearity (THIS REQUIRES THE TIMESERIES OBJECT VERSION OF OUR DATA)\n",
    "    print('Calculating non linear test')\n",
    "    non_linear_test = rtseries.terasvirta_test_ts(x_ts_contiguous,type = \"Chisq\")\n",
    "    '''\n",
    "    x = r.ts(r.rnorm(n=30)) # simulate the time series\n",
    "    non_linear_test = rtseries.terasvirta_test_ts(x,type = \"Chisq\")\n",
    "    print(non_linear_test.names)\n",
    "    [1] \"statistic\" \"parameter\" \"p.value\"   \"method\"    \"data.name\" \"arguments\"\n",
    "    '''\n",
    "    # Grab the first value as seen above\n",
    "    p = list(non_linear_test[0])[0]\n",
    "    fp = f1_transformation(p,0.069,2.304)\n",
    "    \n",
    "    print('Calculating skew + kurtosis')\n",
    "    # Skewness\n",
    "    skew = abs(np.mean((x.dropna()-xbar) ** 3)/std ** 3)\n",
    "    fs = f1_transformation(skew,1.510,5.993)\n",
    "    \n",
    "    # Kurtosis\n",
    "    kurtosis = np.mean((x.dropna()-xbar) ** 4)/std ** 4\n",
    "    fk = f1_transformation(kurtosis,2.273,11567)\n",
    "    \n",
    "    # Hurst=d+0.5 where d is fractional difference\n",
    "    print('Calculting hurst')\n",
    "    hurst = rfracdiff.fracdiff(x_ts_contiguous,0,0)\n",
    "    '''\n",
    "    x = r.ts(r.rnorm(n=30)) # simulate the time series\n",
    "    hurst = rfracdiff.fracdiff(x_ts_contiguous,0,0)\n",
    "    print(hurst.names)\n",
    "     [1] \"log.likelihood\"  \"n\"               \"msg\"             \"d\"              \n",
    "     [5] \"ar\"              \"ma\"              \"covariance.dpq\"  \"fnormMin\"       \n",
    "     [9] \"sigma\"           \"stderror.dpq\"    \"correlation.dpq\" \"h\"              \n",
    "    [13] \"d.tol\"           \"M\"               \"hessian.dpq\"     \"length.w\"       \n",
    "    [17] \"residuals\"       \"fitted\"          \"call\"     \n",
    "    '''\n",
    "    # Grab the fourth value in the hurst variable\n",
    "    H = list(hurst[3])[0] + 0.5\n",
    "    \n",
    "    # Lyapunov Exponent\n",
    "    print('Calculating Lyapunov Exponent')\n",
    "    '''\n",
    "    if freq > (N-10):\n",
    "        # There is insufficient data, declare this variable as none\n",
    "        fLyap = None\n",
    "    else:\n",
    "        Ly = np.zeros(N-freq)\n",
    "        for i in range(0,(N-freq)):\n",
    "            diffs = abs(x.iloc[i] - x)\n",
    "            date_idx = diffs.sort_values().index\n",
    "            int_idx = pd.Index([diffs.index.get_loc(date) for date in date_idx])\n",
    "            idx = int_idx[int_idx < (N-freq)]\n",
    "            j = idx[1]\n",
    "            try:\n",
    "                Ly[i] = math.log(abs((x.iloc[i+freq] - x.iloc[j+freq])/(x.iloc[i]-x.iloc[j]))) / freq\n",
    "            except ValueError: # domain error, means log(0) was taken\n",
    "                Ly[i] = 0\n",
    "            if(np.isnan(Ly[i]) or (Ly[i] == np.Inf) or (Ly[i] == -np.Inf)):\n",
    "                Ly[i] = np.nan\n",
    "        Lyap = np.mean(Ly[~np.isnan(Ly)])\n",
    "        fLyap = math.exp(Lyap) / (1+math.exp(Lyap))\n",
    "    '''\n",
    "    fLyap = None\n",
    "    \n",
    "    measures = measures + [fQ,fp,fs,fk,H,fLyap]\n",
    "    \n",
    "    # Measures on adjusted data\n",
    "    print('Calculating adjusted data measures')\n",
    "    xbar = np.mean(t_adj_x.dropna())\n",
    "    std = np.std(t_adj_x.dropna())\n",
    "\n",
    "    # Serial correlation (method 1)\n",
    "    max_lag = 10\n",
    "    bp = boxpierce(adj_x, lags=max_lag)\n",
    "    Q = bp / (N*max_lag)\n",
    "    fQ = f2_transformation(Q,7.53,0.103)\n",
    "    # Serial correlation (make sure box pierce statistic is returned as well)\n",
    "    # Method 2, but ends up never finishing :(\n",
    "    #lbvalue, pvalue, bpvalue, bppvalue = acorr_ljungbox(na_contiguous(adj_x), lags=max_lag, boxpierce=True)\n",
    "    # The above returns values for each lag, so just grab the final value\n",
    "    #Q = bpvalue[-1] / (N*max_lag)\n",
    "    #fQ = f2_transformation(Q,7.53,0.103)\n",
    "\n",
    "    # Nonlinearity (add try/except block to capture USER IDs where this doesn't work)\n",
    "    # (THIS REQUIRES THE TIMESERIES OBJECT VERSION OF OUR DATA)\n",
    "    adj_x_contiguous = r.ts(FloatVector(na_contiguous(adj_x)))\n",
    "    non_linear_test = rtseries.terasvirta_test_ts(adj_x_contiguous,type = \"Chisq\")\n",
    "    \n",
    "    # Grab first element like we did for untransformed data\n",
    "    p = list(non_linear_test[0])[0]\n",
    "    fp = f1_transformation(p,0.069,2.304)\n",
    "    \n",
    "    # Skewness\n",
    "    skew = abs(np.mean((t_adj_x.dropna() - xbar) ** 3)/(std ** 3))\n",
    "    fs = f1_transformation(skew,1.510,5.993)\n",
    "\n",
    "    # Kurtosis\n",
    "    kurtosis = np.mean((t_adj_x.dropna() - xbar) ** 4)/(std ** 4)\n",
    "    fk = f1_transformation(kurtosis,2.273,11567)\n",
    "    \n",
    "    measures_list = measures + [fQ,fp,fs,fk]\n",
    "\n",
    "    return measures_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and Combine WESAD Data\n",
    "\n",
    "The WESAD data came in zipped files by subject, with processed data pickled together and unprocessed data contained within the same zipped file per subject.\n",
    "\n",
    "Here I will grab the processed data and create a dictionary of subject data by subject id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing subject S5\n",
      "processing subject S2\n",
      "processing subject S3\n",
      "processing subject S4\n",
      "processing subject S17\n",
      "processing subject S10\n",
      "processing subject S11\n",
      "processing subject S16\n",
      "processing subject S8\n",
      "processing subject S6\n",
      "processing subject S7\n",
      "processing subject S9\n",
      "processing subject S13\n",
      "processing subject S14\n",
      "processing subject S15\n"
     ]
    }
   ],
   "source": [
    "# This is from the data_etl.py script\n",
    "# Posting the format of the data below from the wesad_readme.pdf\n",
    "\"\"\"\n",
    "According to the README:\n",
    "The double-tap signal pattern was used to manually synchronise the two devices' raw data. The result is provided in the files SX.pkl, one file per subject. This file is a dictionary, with the following keys:\n",
    "- 'subject': SX, the subject ID\n",
    "- 'signal': includes all the raw data, in two fields:\n",
    "  - 'chest': RespiBAN data (all the modalities: ACC, ECG, EDA, EMG, RESP, TEMP)\n",
    "  - 'wrist':EmpaticaE4data(all the modalities:ACC,BVP,EDA,TEMP)\n",
    "- 'label': ID of the respective study protocol condition, sampled at 700 Hz. The following IDs\n",
    "are provided: 0 = not defined / transient, 1 = baseline, 2 = stress, 3 = amusement, 4 = meditation, 5/6/7 = should be ignored in this dataset\n",
    "\"\"\"\n",
    "\n",
    "# Study protocal conditions (label) mapping\n",
    "label_map = {\n",
    "    0: 'not defined / transient',\n",
    "    1: 'baseline',\n",
    "    2: 'stress',\n",
    "    3: 'amusement',\n",
    "    4: 'meditation',\n",
    "}\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "# Read in WESAD datasets by subject and unpickle\n",
    "subject_dct = {}\n",
    "path = '../../data/WESAD'\n",
    "filenames = glob.glob(os.path.join(path,'*/*.pkl'))\n",
    "for file in filenames:\n",
    "    \n",
    "    # Had to use 'latin1' as the encoding due to Python 2/3 pickle incompatibility\n",
    "    # https://stackoverflow.com/questions/11305790/pickle-incompatibility-of-numpy-arrays-between-python-2-and-3\n",
    "    unpickled_file = pickle.load(open(file,'rb'), encoding='latin1')\n",
    "    # Grab relevant info\n",
    "    subject_id = unpickled_file['subject']\n",
    "    print('processing subject',subject_id)\n",
    "    # Grab chest and wrist dataframes\n",
    "    chest_dct = unpickled_file['signal']['chest']\n",
    "    wrist_dct = unpickled_file['signal']['wrist']\n",
    "\n",
    "    # Process the chest dictionary first as it is more straight forward\n",
    "    # Since the 'ACC' column contains 3 dimensional tuples, it needs to be processed separately due to pandas expecting the same format for all columns\n",
    "    # Going to create dictionaries without that column to turn into a dataframe, then add the 'ACC' values later\n",
    "    tmp_chest_dct = dict((k, chest_dct[k].ravel()) for k in list(chest_dct.keys()) if k not in ['ACC'])\n",
    "    tmp_chest_df = pd.DataFrame(tmp_chest_dct) # Contains everything except ACC\n",
    "    tmp_acc_df = pd.DataFrame(chest_dct['ACC'],columns=['ACC_X','ACC_Y','ACC_Z']) # Manually declare keys, otherwise shows up as 0,1,2\n",
    "    final_chest_df = pd.concat([tmp_chest_df,tmp_acc_df],axis=1)\n",
    "\n",
    "    # Process wrist dictionary, which will take more care because the samplying frequencies were different \n",
    "    # Meaning the number of data points collected for each feature is different (higher frequency equals more data points)\n",
    "    # Basically this one just needs to be processed manually\n",
    "    wrist_acc_df = pd.DataFrame(wrist_dct['ACC'],columns=['ACC_X','ACC_Y','ACC_Z'])\n",
    "    wrist_bvp_df = pd.DataFrame(wrist_dct['BVP'],columns=['BVP'])\n",
    "    wrist_eda_df = pd.DataFrame(wrist_dct['EDA'],columns=['EDA'])\n",
    "    wrist_temp_df = pd.DataFrame(wrist_dct['TEMP'],columns=['TEMP'])\n",
    "\n",
    "    # Add labels as a separate object to be returned\n",
    "    # While the time granularity is the same as the chest data, I'm not sure yet how to use it with the wrist data\n",
    "    # So will just keep it separate and add as needed\n",
    "    labels_df = pd.DataFrame(unpickled_file['label'],columns=['label'])\n",
    "    labels_df['mapped_label'] = labels_df['label'].map(label_map)\n",
    "    labels_df['SUBJECT_ID'] = subject_id\n",
    "    \n",
    "    # Add subject id to all dataframes\n",
    "    for df in [final_chest_df, wrist_acc_df, wrist_bvp_df, wrist_eda_df, wrist_temp_df]:\n",
    "        df['SUBJECT_ID'] = subject_id\n",
    "\n",
    "    subject_dct[subject_id] = {\n",
    "        'chest_df': final_chest_df,\n",
    "        'wrist_dfs': {\n",
    "            'wrist_acc_df': wrist_acc_df,\n",
    "            'wrist_bvp_df': wrist_bvp_df,\n",
    "            'wrist_eda_df': wrist_eda_df,\n",
    "            'wrist_temp_df': wrist_temp_df,\n",
    "        },\n",
    "        'labels': labels_df,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chest Data + Feature Extraction\n",
    "\n",
    "According to the README all data was sampled at 700 Hertz. In order to get the period from this, we take the reciprocal of the frequency, which is 1/700. However, looking at the docs for the [sm.tsa.seasonal_decompose](https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html) and [pandas DatetimeIndex](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html) functions, it won't accept a non integer period value. So I guess we have to input the period as 1..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ECG</th>\n",
       "      <th>EMG</th>\n",
       "      <th>EDA</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Resp</th>\n",
       "      <th>ACC_X</th>\n",
       "      <th>ACC_Y</th>\n",
       "      <th>ACC_Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.949700e+06</td>\n",
       "      <td>4.949700e+06</td>\n",
       "      <td>4.949700e+06</td>\n",
       "      <td>4.949700e+06</td>\n",
       "      <td>4.949700e+06</td>\n",
       "      <td>4.949700e+06</td>\n",
       "      <td>4.949700e+06</td>\n",
       "      <td>4.949700e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.865652e-04</td>\n",
       "      <td>-3.929084e-03</td>\n",
       "      <td>8.292531e+00</td>\n",
       "      <td>3.396810e+01</td>\n",
       "      <td>5.261134e-02</td>\n",
       "      <td>7.101258e-01</td>\n",
       "      <td>-5.102462e-02</td>\n",
       "      <td>-4.128740e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.035570e-01</td>\n",
       "      <td>1.791825e-02</td>\n",
       "      <td>1.534487e+00</td>\n",
       "      <td>3.257616e-01</td>\n",
       "      <td>4.119917e+00</td>\n",
       "      <td>1.245003e-01</td>\n",
       "      <td>5.831174e-02</td>\n",
       "      <td>4.362070e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-7.138824e-01</td>\n",
       "      <td>-4.449463e-01</td>\n",
       "      <td>5.171204e+00</td>\n",
       "      <td>3.287875e+01</td>\n",
       "      <td>-3.152313e+01</td>\n",
       "      <td>2.490000e-01</td>\n",
       "      <td>-6.728000e-01</td>\n",
       "      <td>-3.468600e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-9.095764e-02</td>\n",
       "      <td>-1.158142e-02</td>\n",
       "      <td>7.506561e+00</td>\n",
       "      <td>3.372012e+01</td>\n",
       "      <td>-2.288818e+00</td>\n",
       "      <td>5.962000e-01</td>\n",
       "      <td>-8.520001e-02</td>\n",
       "      <td>-7.426000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.494812e-02</td>\n",
       "      <td>-3.936768e-03</td>\n",
       "      <td>8.231735e+00</td>\n",
       "      <td>3.398135e+01</td>\n",
       "      <td>-1.205444e-01</td>\n",
       "      <td>7.006000e-01</td>\n",
       "      <td>-4.439998e-02</td>\n",
       "      <td>-6.246000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.009583e-02</td>\n",
       "      <td>3.524780e-03</td>\n",
       "      <td>9.099960e+00</td>\n",
       "      <td>3.421146e+01</td>\n",
       "      <td>2.381897e+00</td>\n",
       "      <td>8.290000e-01</td>\n",
       "      <td>-1.639998e-02</td>\n",
       "      <td>-2.286000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.499954e+00</td>\n",
       "      <td>4.730988e-01</td>\n",
       "      <td>1.225357e+01</td>\n",
       "      <td>3.522610e+01</td>\n",
       "      <td>3.880005e+01</td>\n",
       "      <td>1.844200e+00</td>\n",
       "      <td>5.590000e-01</td>\n",
       "      <td>2.284200e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ECG           EMG           EDA          Temp          Resp  \\\n",
       "count  4.949700e+06  4.949700e+06  4.949700e+06  4.949700e+06  4.949700e+06   \n",
       "mean   6.865652e-04 -3.929084e-03  8.292531e+00  3.396810e+01  5.261134e-02   \n",
       "std    3.035570e-01  1.791825e-02  1.534487e+00  3.257616e-01  4.119917e+00   \n",
       "min   -7.138824e-01 -4.449463e-01  5.171204e+00  3.287875e+01 -3.152313e+01   \n",
       "25%   -9.095764e-02 -1.158142e-02  7.506561e+00  3.372012e+01 -2.288818e+00   \n",
       "50%   -2.494812e-02 -3.936768e-03  8.231735e+00  3.398135e+01 -1.205444e-01   \n",
       "75%    2.009583e-02  3.524780e-03  9.099960e+00  3.421146e+01  2.381897e+00   \n",
       "max    1.499954e+00  4.730988e-01  1.225357e+01  3.522610e+01  3.880005e+01   \n",
       "\n",
       "              ACC_X         ACC_Y         ACC_Z  \n",
       "count  4.949700e+06  4.949700e+06  4.949700e+06  \n",
       "mean   7.101258e-01 -5.102462e-02 -4.128740e-01  \n",
       "std    1.245003e-01  5.831174e-02  4.362070e-01  \n",
       "min    2.490000e-01 -6.728000e-01 -3.468600e+00  \n",
       "25%    5.962000e-01 -8.520001e-02 -7.426000e-01  \n",
       "50%    7.006000e-01 -4.439998e-02 -6.246000e-01  \n",
       "75%    8.290000e-01 -1.639998e-02 -2.286000e-01  \n",
       "max    1.844200e+00  5.590000e-01  2.284200e+00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_dct['S6']['chest_df'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now calculating chest measures for user  S5\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n",
      "Now calculating chest measures for user  S2\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n",
      "Now calculating chest measures for user  S3\n",
      "Frequency calculated\n",
      "Code hit an error for the following reason:\n",
      " Data must be positive.\n",
      "Now calculating chest measures for user  S4\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n",
      "Now calculating chest measures for user  S17\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n",
      "Now calculating chest measures for user  S10\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Code hit an error for the following reason:\n",
      " math range error\n",
      "Now calculating chest measures for user  S11\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n",
      "Now calculating chest measures for user  S16\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n",
      "Now calculating chest measures for user  S8\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n",
      "Now calculating chest measures for user  S6\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n",
      "Now calculating chest measures for user  S7\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n",
      "Now calculating chest measures for user  S9\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n",
      "Now calculating chest measures for user  S13\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n",
      "Now calculating chest measures for user  S14\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Code hit an error for the following reason:\n",
      " math range error\n",
      "Now calculating chest measures for user  S15\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n"
     ]
    }
   ],
   "source": [
    "# Pick a column we want to analyze\n",
    "# Some of the columns won't make sense to cluster, such as accelerometer\n",
    "analysis_col = 'EDA'\n",
    "chest_measures_dct = {}\n",
    "for id,dfs in subject_dct.items():\n",
    "    print('Now calculating chest measures for user ',id)\n",
    "    chest_df = dfs['chest_df'][analysis_col]\n",
    "    try:\n",
    "        chest_measures_dct[id] = calculate_measures(chest_df)\n",
    "    except Exception as e:\n",
    "        print('Code hit an error for the following reason:\\n',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S3', 'S10', 'S14']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see for which subjects the above code did not work for whatever reason\n",
    "[key for key in subject_dct.keys() if key not in chest_measures_dct.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>trend</th>\n",
       "      <th>seasonal</th>\n",
       "      <th>autocorrelation</th>\n",
       "      <th>non-linear</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>Hurst</th>\n",
       "      <th>Lyapunov</th>\n",
       "      <th>dc autocorrelation</th>\n",
       "      <th>dc non-linear</th>\n",
       "      <th>dc skewness</th>\n",
       "      <th>dc kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84.016253</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.006312</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.002234</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4010.111120</td>\n",
       "      <td>0.827380</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5847.291864</td>\n",
       "      <td>0.390720</td>\n",
       "      <td>0.139011</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002052</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>549.117971</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1382.164930</td>\n",
       "      <td>0.475247</td>\n",
       "      <td>0.716278</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>735.898481</td>\n",
       "      <td>0.406702</td>\n",
       "      <td>0.289871</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2321.617123</td>\n",
       "      <td>0.698201</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.002416</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.728807</td>\n",
       "      <td>0.080102</td>\n",
       "      <td>0.044070</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.791605</td>\n",
       "      <td>0.437424</td>\n",
       "      <td>0.089567</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>778.194924</td>\n",
       "      <td>0.057255</td>\n",
       "      <td>0.010862</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>737.067812</td>\n",
       "      <td>0.458090</td>\n",
       "      <td>0.927581</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.001724</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>568.563166</td>\n",
       "      <td>0.055147</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     frequency  trend  seasonal  autocorrelation   non-linear  skewness  \\\n",
       "S5         0.0      1         0              1.0    84.016253  0.002917   \n",
       "S2         0.0      1         0              1.0  4010.111120  0.827380   \n",
       "S4         0.0      1         0              1.0  5847.291864  0.390720   \n",
       "S17        0.0      1         0              1.0   549.117971  0.001004   \n",
       "S11        0.0      0         0              1.0  1382.164930  0.475247   \n",
       "S16        0.0      1         0              1.0   735.898481  0.406702   \n",
       "S8         0.0      0         0              1.0  2321.617123  0.698201   \n",
       "S6         0.0      1         0              1.0    18.728807  0.080102   \n",
       "S7         0.0      1         0              1.0    59.791605  0.437424   \n",
       "S9         0.0      1         0              1.0   778.194924  0.057255   \n",
       "S13        0.0      1         0              1.0   737.067812  0.458090   \n",
       "S15        0.0      1         0              1.0   568.563166  0.055147   \n",
       "\n",
       "     kurtosis     Hurst Lyapunov  dc autocorrelation  dc non-linear  \\\n",
       "S5   0.006312  0.999954     None                 1.0      -0.002234   \n",
       "S2   0.999915  0.999954     None                 1.0       0.001490   \n",
       "S4   0.139011  0.999954     None                 1.0       0.002052   \n",
       "S17  0.002709  0.999954     None                 1.0       0.000308   \n",
       "S11  0.716278  0.999954     None                 1.0       0.000021   \n",
       "S16  0.289871  0.999954     None                 1.0       0.001037   \n",
       "S8   0.999985  0.999954     None                 1.0      -0.002416   \n",
       "S6   0.044070  0.999954     None                 1.0      -0.001148   \n",
       "S7   0.089567  0.999954     None                 1.0       0.000187   \n",
       "S9   0.010862  0.999954     None                 1.0      -0.000024   \n",
       "S13  0.927581  0.999954     None                 1.0      -0.001724   \n",
       "S15  0.004663  0.999954     None                 1.0       0.002373   \n",
       "\n",
       "     dc skewness  dc kurtosis  \n",
       "S5      0.335249     0.000752  \n",
       "S2      0.335249     0.000752  \n",
       "S4      0.335249     0.000752  \n",
       "S17     0.335249     0.000752  \n",
       "S11     0.335249     0.000752  \n",
       "S16     0.335249     0.000752  \n",
       "S8      0.335249     0.000752  \n",
       "S6      0.335249     0.000752  \n",
       "S7      0.335249     0.000752  \n",
       "S9      0.335249     0.000752  \n",
       "S13     0.335249     0.000752  \n",
       "S15     0.335249     0.000752  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Not too bad, let's inspect our features from the data we DID get\n",
    "chest_measures_df = pd.DataFrame.from_dict(chest_measures_dct,orient='index',columns=[\"frequency\", \"trend\",\"seasonal\", \"autocorrelation\",\"non-linear\",\"skewness\",\"kurtosis\",\"Hurst\",\"Lyapunov\",\"dc autocorrelation\",\"dc non-linear\",\"dc skewness\",\"dc kurtosis\"])\n",
    "display(chest_measures_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrist Data + Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now calculating wrist measures for user  S5\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-ccc57babc47d>:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  skew = abs(np.mean((t_adj_x.dropna() - xbar) ** 3)/(std ** 3))\n",
      "<ipython-input-6-ccc57babc47d>:192: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  kurtosis = np.mean((t_adj_x.dropna() - xbar) ** 4)/(std ** 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now calculating wrist measures for user  S2\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfritter/workspace/fitbit/data-science/gitRepos/Characteristic-Based-Time-Series-Clustering/venv/lib/python3.8/site-packages/pypr/stattest/ljungbox.py:38: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return N/D\n",
      "R[write to console]: Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : \n",
      "  0 (non-NA) cases\n",
      "\n",
      "R[write to console]: In addition: \n",
      "R[write to console]: There were 11 warnings (use warnings() to see them)\n",
      "R[write to console]: \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n",
      "\n",
      "Following exception occurred:\n",
      " Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : \n",
      "  0 (non-NA) cases\n",
      "\n",
      "Now calculating wrist measures for user  S3\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : \n",
      "  0 (non-NA) cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Following exception occurred:\n",
      " Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : \n",
      "  0 (non-NA) cases\n",
      "\n",
      "Now calculating wrist measures for user  S4\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfritter/workspace/fitbit/data-science/gitRepos/Characteristic-Based-Time-Series-Clustering/venv/lib/python3.8/site-packages/pypr/stattest/ljungbox.py:38: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return N/D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now calculating wrist measures for user  S17\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-ccc57babc47d>:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  skew = abs(np.mean((t_adj_x.dropna() - xbar) ** 3)/(std ** 3))\n",
      "<ipython-input-6-ccc57babc47d>:192: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  kurtosis = np.mean((t_adj_x.dropna() - xbar) ** 4)/(std ** 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now calculating wrist measures for user  S10\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-ccc57babc47d>:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  skew = abs(np.mean((t_adj_x.dropna() - xbar) ** 3)/(std ** 3))\n",
      "<ipython-input-6-ccc57babc47d>:192: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  kurtosis = np.mean((t_adj_x.dropna() - xbar) ** 4)/(std ** 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now calculating wrist measures for user  S11\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfritter/workspace/fitbit/data-science/gitRepos/Characteristic-Based-Time-Series-Clustering/venv/lib/python3.8/site-packages/pypr/stattest/ljungbox.py:38: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return N/D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now calculating wrist measures for user  S16\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : \n",
      "  0 (non-NA) cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Following exception occurred:\n",
      " Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : \n",
      "  0 (non-NA) cases\n",
      "\n",
      "Now calculating wrist measures for user  S8\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : \n",
      "  0 (non-NA) cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n",
      "\n",
      "Following exception occurred:\n",
      " Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : \n",
      "  0 (non-NA) cases\n",
      "\n",
      "Now calculating wrist measures for user  S6\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n",
      "Now calculating wrist measures for user  S7\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfritter/workspace/fitbit/data-science/gitRepos/Characteristic-Based-Time-Series-Clustering/venv/lib/python3.8/site-packages/pypr/stattest/ljungbox.py:38: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return N/D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now calculating wrist measures for user  S9\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n",
      "Now calculating wrist measures for user  S13\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfritter/workspace/fitbit/data-science/gitRepos/Characteristic-Based-Time-Series-Clustering/venv/lib/python3.8/site-packages/pypr/stattest/ljungbox.py:38: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return N/D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now calculating wrist measures for user  S14\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : \n",
      "  0 (non-NA) cases\n",
      "\n",
      "R[write to console]: In addition: \n",
      "R[write to console]: Warning message:\n",
      "\n",
      "R[write to console]: unable to compute correlation matrix; maybe change 'h' \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Following exception occurred:\n",
      " Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : \n",
      "  0 (non-NA) cases\n",
      "\n",
      "Now calculating wrist measures for user  S15\n",
      "Frequency calculated\n",
      "Decomposition complete\n",
      "Adj x complete\n",
      "Calculating trend/seasonal measures\n",
      "Calculating serial correlation\n",
      "Calculating non linear test\n",
      "Calculating skew + kurtosis\n",
      "Calculting hurst\n",
      "Calculating Lyapunov Exponent\n",
      "Calculating adjusted data measures\n"
     ]
    }
   ],
   "source": [
    "# Pick a column we want to analyze\n",
    "# Some of the columns won't make sense to cluster, such as accelerometer\n",
    "analysis_col = 'EDA'\n",
    "errors = {}\n",
    "wrist_measures_dct = {}\n",
    "for id,dfs in subject_dct.items(): \n",
    "    print('Now calculating wrist measures for user ',id)\n",
    "    wrist_total_df = pd.concat([dfs['wrist_dfs'][key] for key in dfs['wrist_dfs']],axis=1)\n",
    "    try:\n",
    "        wrist_measures_dct[id] = calculate_measures(wrist_total_df[analysis_col])\n",
    "    except Exception as e:\n",
    "        print('\\nFollowing exception occurred:\\n',e)\n",
    "        errors[id] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S2': rpy2.rinterface_lib.embedded.RRuntimeError('Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : \\n  0 (non-NA) cases\\n'),\n",
       " 'S3': rpy2.rinterface_lib.embedded.RRuntimeError('Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : \\n  0 (non-NA) cases\\n'),\n",
       " 'S16': rpy2.rinterface_lib.embedded.RRuntimeError('Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : \\n  0 (non-NA) cases\\n'),\n",
       " 'S8': rpy2.rinterface_lib.embedded.RRuntimeError('Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : \\n  0 (non-NA) cases\\n'),\n",
       " 'S14': rpy2.rinterface_lib.embedded.RRuntimeError('Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : \\n  0 (non-NA) cases\\n')}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check how many subjects incurred errors\n",
    "display(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>trend</th>\n",
       "      <th>seasonal</th>\n",
       "      <th>autocorrelation</th>\n",
       "      <th>non-linear</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>Hurst</th>\n",
       "      <th>Lyapunov</th>\n",
       "      <th>dc autocorrelation</th>\n",
       "      <th>dc non-linear</th>\n",
       "      <th>dc skewness</th>\n",
       "      <th>dc kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026349</td>\n",
       "      <td>2.311066</td>\n",
       "      <td>0.507419</td>\n",
       "      <td>0.486695</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>0.352655</td>\n",
       "      <td>5.572116e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026364</td>\n",
       "      <td>44.481694</td>\n",
       "      <td>0.804111</td>\n",
       "      <td>0.996398</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.132397e-09</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026201</td>\n",
       "      <td>21.400773</td>\n",
       "      <td>0.856079</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>0.352640</td>\n",
       "      <td>-5.460976e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026357</td>\n",
       "      <td>3.165258</td>\n",
       "      <td>0.473951</td>\n",
       "      <td>0.281150</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>0.352620</td>\n",
       "      <td>-5.308065e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026317</td>\n",
       "      <td>13.016022</td>\n",
       "      <td>0.434946</td>\n",
       "      <td>0.231958</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.905507e-08</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026365</td>\n",
       "      <td>6.650583</td>\n",
       "      <td>0.260931</td>\n",
       "      <td>0.028975</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>0.352684</td>\n",
       "      <td>-8.482500e-08</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026357</td>\n",
       "      <td>37.922325</td>\n",
       "      <td>0.272355</td>\n",
       "      <td>0.023140</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.619502e-08</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026357</td>\n",
       "      <td>140.871185</td>\n",
       "      <td>0.283348</td>\n",
       "      <td>0.027142</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>0.352605</td>\n",
       "      <td>3.664775e-10</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026363</td>\n",
       "      <td>17.277814</td>\n",
       "      <td>0.358343</td>\n",
       "      <td>0.088192</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-8.852119e-10</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026344</td>\n",
       "      <td>1.635552</td>\n",
       "      <td>0.334374</td>\n",
       "      <td>0.055538</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>None</td>\n",
       "      <td>0.352606</td>\n",
       "      <td>-4.240224e-09</td>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     frequency  trend  seasonal  autocorrelation  non-linear  skewness  \\\n",
       "S5         0.0      1         0         0.026349    2.311066  0.507419   \n",
       "S4         0.0      1         0         0.026364   44.481694  0.804111   \n",
       "S17        0.0      1         0         0.026201   21.400773  0.856079   \n",
       "S10        0.0      1         0         0.026357    3.165258  0.473951   \n",
       "S11        0.0      1         0         0.026317   13.016022  0.434946   \n",
       "S6         0.0      1         0         0.026365    6.650583  0.260931   \n",
       "S7         0.0      1         0         0.026357   37.922325  0.272355   \n",
       "S9         0.0      1         0         0.026357  140.871185  0.283348   \n",
       "S13        0.0      1         0         0.026363   17.277814  0.358343   \n",
       "S15        0.0      1         0         0.026344    1.635552  0.334374   \n",
       "\n",
       "     kurtosis     Hurst Lyapunov  dc autocorrelation  dc non-linear  \\\n",
       "S5   0.486695  0.999954     None            0.352655   5.572116e-08   \n",
       "S4   0.996398  0.999954     None                 NaN  -1.132397e-09   \n",
       "S17  0.999997  0.999954     None            0.352640  -5.460976e-08   \n",
       "S10  0.281150  0.999954     None            0.352620  -5.308065e-08   \n",
       "S11  0.231958  0.999954     None                 NaN  -7.905507e-08   \n",
       "S6   0.028975  0.999954     None            0.352684  -8.482500e-08   \n",
       "S7   0.023140  0.999954     None                 NaN  -7.619502e-08   \n",
       "S9   0.027142  0.999954     None            0.352605   3.664775e-10   \n",
       "S13  0.088192  0.999954     None                 NaN  -8.852119e-10   \n",
       "S15  0.055538  0.999954     None            0.352606  -4.240224e-09   \n",
       "\n",
       "     dc skewness  dc kurtosis  \n",
       "S5           NaN          NaN  \n",
       "S4      0.335249     0.000752  \n",
       "S17          NaN          NaN  \n",
       "S10          NaN          NaN  \n",
       "S11     0.335249     0.000752  \n",
       "S6      0.335249     0.000752  \n",
       "S7      0.335249     0.000752  \n",
       "S9      0.335249     0.000752  \n",
       "S13     0.335249     0.000752  \n",
       "S15     0.335249     0.000752  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Looks like all the same errors, that might be addressable\n",
    "# Let's see our features\n",
    "wrist_measures_df = pd.DataFrame.from_dict(wrist_measures_dct,orient='index',columns=[\"frequency\", \"trend\",\"seasonal\", \"autocorrelation\",\"non-linear\",\"skewness\",\"kurtosis\",\"Hurst\",\"Lyapunov\",\"dc autocorrelation\",\"dc non-linear\",\"dc skewness\",\"dc kurtosis\"])\n",
    "display(wrist_measures_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Scaling (Standardizing) the Raw Features via Alternative Methods\n",
    "\n",
    "Given the guidelines of the paper, certain features should be within certain ranges of values. When this is not feasible, alternative effective scaling methods were also mentioned.\n",
    "\n",
    "The alternative scaling methods (albeit whose purpose was to be be used for comparison in clustering experiments later in the paper):\n",
    "  - A linear transform method, mapping (–∞,∞) to [0,1]   range:\n",
    "\\begin{equation*} vnorm = \\frac{v_i − min(v_1...v_n)}{max(v_1...v_n)−min(v_1...v_n)} \\end{equation*}\n",
    "    - Where vnorm is the normalized value and v_i is a instance value of actual values\n",
    "  - The Softmax scaling (the logistic function) to map (–∞,∞) to (0, 1):\n",
    "\\begin{equation*} v_n = \\frac{1}{1+e^{−v_i}} \\end{equation*}\n",
    "where \\begin{equation*} e^{−v_i} = 1/e^{v_i} \\end{equation*} v_n denotes the normalized value and v_i denotes a instance value of actual values \n",
    "\n",
    "Let's start with the first method, the Linear Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_transformation(df):\n",
    "    # Take in an input dataframe, take each value in every column and apply the linear transformation column wise\n",
    "    # Because pandas operates column wise, we can use the following one liner\n",
    "    linear_transformed_data = ((df - df.min())/(df.max() - df.min()))\n",
    "    if linear_transformed_data['ANALYSIS_PERIODICTY_RAW'].isnull().sum() == len(linear_transformed_data):\n",
    "        # This means all values got coerced to nulls; will set back to 1 here\n",
    "        print('Converting null values back to 1')\n",
    "        linear_transformed_data['ANALYSIS_PERIODICTY_RAW'] = 1\n",
    "\n",
    "    return linear_transformed_data\n",
    "\n",
    "import math\n",
    "def softmax_scaling(df):\n",
    "    # Take in an input dataframe, take each value in every column and apply the softmax transformation column wise\n",
    "    # Because pandas operates column wise, we can use the following one liner\n",
    "    #softmax_scaled_data = df.apply(lambda x: 1 / (1 + math.exp(-pd.to_numeric(x))))\n",
    "    softmax_scaled_data = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        softmax_scaled_data_col = df[col].apply(lambda x: (1 / (1 + math.exp(-x))))\n",
    "        softmax_scaled_data = pd.concat([softmax_scaled_data,softmax_scaled_data_col],axis=1)\n",
    "    return softmax_scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering!!!\n",
    "Finally, to the good stuff!\n",
    "\n",
    "Now that we have our scaled data, let's go ahead and apply some clustering methods to see if the subjects can be clustered together via extracted time series features.\n",
    "\n",
    "According to the paper, two methods were used: heirarchical clustering and self organizing map clustering. Hierachical clutsering is a well known method that has been applied in many domains, and SOMs are robust in parameter selection, natural clustering results, and achieve superior visualization compared to other clustering methods, such as hierarchical clustering and K-means.\n",
    "\n",
    "Let's move forward with both methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierachical Clustering\n",
    "From the paper:\n",
    "\n",
    "\"There are three major variants of hierarchical clustering algorithms: Single-link, complete-link, and minimum-variance algorithms. Of these three, the single-link and complete-link algorithms are most popular.\"\n",
    "\n",
    "\"We have chosen the complete-link hierarchical clustering algorithm to achieve more useful hierarchies than single-link from a pragmatic viewpoint.\"\n",
    "\n",
    "According to python scikit-learn docs (https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering) the four major kinds of hierarchical clustering:\n",
    "  - Ward minimizes the sum of squared differences within all clusters. \n",
    "    - It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n",
    "  - Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters.\n",
    "  - Average linkage minimizes the average of the distances between all observations of pairs of clusters.\n",
    "  - Single linkage minimizes the distance between the closest observations of pairs of clusters.\n",
    "Looks like the \"average linkage\" method gets no love..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def hierarchical_clustering_sklearn(normalized_df,n_clusters=2,affinity='euclidean',linkage='complete'):\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters,affinity=affinity,linkage=linkage).fit(normalized_df)\n",
    "    print('Number of clusters: ',clustering.n_clusters_)\n",
    "    print('Number of leaves: ',clustering.n_leaves_)\n",
    "    print('Number of connected components: ',clustering.n_connected_components_)\n",
    "    return clustering\n",
    "\n",
    "def hierarchical_clustering_scipy(normalized_df,method='complete',metric='euclidean'):\n",
    "    clustering = sch.linkage(normalized_df,method=method,metric=metric)\n",
    "    return clustering\n",
    "\n",
    "def plot_dendrogram_scipy(clustering_model):\n",
    "    dendrogram = sch.dendrogram(clustering_model)\n",
    "    plt.title('Dendrogram')\n",
    "    plt.xlabel('Customers')\n",
    "    plt.ylabel('Euclidean distances')\n",
    "    plt.show()\n",
    "\n",
    "def plot_dendrogram_sklearn(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "    plt.title('Hierarchical Clustering Dendrogram')\n",
    "    # plot the top three levels of the dendrogram\n",
    "    plot_dendrogram(model, truncate_mode='level', p=3)\n",
    "    plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfritter/workspace/fitbit/data-science/gitRepos/Characteristic-Based-Time-Series-Clustering/venv/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "CACHEDIR=/Users/nfritter/.matplotlib\n",
      "Using fontManager instance from /Users/nfritter/.matplotlib/fontlist-v330.json\n",
      "Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "# Going to use the approach from https://github.com/hhl60492/SOMPY_robust_clustering/blob/master/sompy/examples/main.py\n",
    "# As well as apply k means clustering to the clusters generated by the above\n",
    "# Also had to do some workarounds to get sompy package to work: https://github.com/sevamoo/SOMPY/issues/36\n",
    "# Needed to do the following to get this to work:\n",
    "# pip3 install git+https://github.com/compmonks/SOMPY.git\n",
    "# pip3 install ipdb==0.8.1\n",
    "from sompy.sompy import SOMFactory\n",
    "from sompy.visualization.mapview import View2D\n",
    "from sompy.visualization.umatrix import UMatrixView\n",
    "from sompy.visualization.hitmap import HitMapView\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "def self_organizing_map(normalized_df,normalization='var',initialization='pca',n_job=1,train_rough_len=2,train_finetune_len=5,verbose=None):\n",
    "    # create the SOM network and train it. You can experiment with different normalizations and initializations\n",
    "    som = SOMFactory().build(normalized_df.values,normalization=normalization,initialization=initialization,component_names=normalized_df.columns)\n",
    "    som.train(n_job=n_job,train_rough_len=train_rough_len,train_finetune_len=train_finetune_len,verbose=verbose)\n",
    "    \n",
    "    # The quantization error: average distance between each data vector and its BMU.\n",
    "    # The topographic error: the proportion of all data vectors for which first and second BMUs are not adjacent units.\n",
    "    topographic_error = som.calculate_topographic_error()\n",
    "    quantization_error = np.mean(som._bmu[1])\n",
    "    print(\"Topographic error = %s; Quantization error = %s\" % (topographic_error, quantization_error))\n",
    "    return som\n",
    "\n",
    "def som_component_planes(som):\n",
    "    # component planes view\n",
    "    view2D = View2D(15,15,\"rand data\",text_size=12)\n",
    "    view2D.show(som, col_sz=4, which_dim=\"all\", desnormalize=True)\n",
    "    plt.show()\n",
    "\n",
    "def som_kmeans_clustering_predict(som,k):\n",
    "    # This performed K-means clustering with k clusters on the SOM grid to PREDICT clusters\n",
    "    #[labels, km, norm_data] = som.cluster(K,K_opt)\n",
    "    map_labels = som.cluster(n_clusters=k)\n",
    "    data_labels = np.array([map_labels[int(k)] for k in som._bmu[0]])\n",
    "    hits = HitMapView(20,20,\"Clustering\",text_size=12)\n",
    "    a=hits.show(som)\n",
    "    return som,map_labels\n",
    "\n",
    "def som_kmeans_clustering(som,k):\n",
    "    # Perform K Means clustering on the SOM grid just FITTING data so we can get more data returned\n",
    "    kMeansCluster = KMeans(n_clusters=k).fit(\n",
    "        som._normalizer.denormalize_by(som.data_raw,\n",
    "                                        som.codebook.matrix))\n",
    "    return kMeansCluster\n",
    "\n",
    "def som_u_matrix(som):\n",
    "    # U-matrix plot\n",
    "    umat = UMatrixView(width=10,height=10,title='U-matrix')\n",
    "    umat.show(som)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-588cacaa2079>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m## Hierarchical Clustering (on regular data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mchest_clustering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhierarchical_clustering_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchest_measures_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mwrist_clustering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhierarchical_clustering_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrist_measures_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-f83e07afde1b>\u001b[0m in \u001b[0;36mhierarchical_clustering_sklearn\u001b[0;34m(normalized_df, n_clusters, affinity, linkage)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhierarchical_clustering_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maffinity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mclustering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgglomerativeClustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maffinity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maffinity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of clusters: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of leaves: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_leaves_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/fitbit/data-science/gitRepos/Characteristic-Based-Time-Series-Clustering/venv/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \"\"\"\n\u001b[0;32m--> 796\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/fitbit/data-science/gitRepos/Characteristic-Based-Time-Series-Clustering/venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    578\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/fitbit/data-science/gitRepos/Characteristic-Based-Time-Series-Clustering/venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     56\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "## Clustering parameters\n",
    "# Hierarchical Clustering parameters\n",
    "n_clusters = 3 # Number of clusters for clustering\n",
    "linkage = 'complete' # Type of hierarchical clustering method\n",
    "affinity = 'euclidean' # \"Proximity\" Measure for hierarchical clustering\n",
    "\n",
    "# Self Organizing Map parameters\n",
    "normalization = 'var'\n",
    "initialization = 'pca'\n",
    "n_job = 2\n",
    "verbose = 'info'\n",
    "train_rough_len = 1\n",
    "train_finetune_len = 5\n",
    "\n",
    "## Hierarchical Clustering (on regular data)\n",
    "chest_clustering = hierarchical_clustering_sklearn(chest_measures_df,n_clusters=n_clusters,linkage=linkage)\n",
    "wrist_clustering = hierarchical_clustering_sklearn(wrist_measures_df,n_clusters=n_clusters,linkage=linkage)\n",
    "\n",
    "# Self Organizing Map Grid Creation (on regular data)\n",
    "chest_som = self_organizing_map(chest_measures_df,normalization=normalization,initialization=initialization,n_job=n_job,train_rough_len=train_rough_len,train_finetune_len=train_finetune_len,verbose=verbose)\n",
    "wrist_som = self_organizing_map(wrist_measures_df,normalization=normalization,initialization=initialization,n_job=n_job,train_rough_len=train_rough_len,train_finetune_len=train_finetune_len,verbose=verbose)\n",
    "\n",
    "## Hierarchical Clustering (Using Linear + Softmax Transformations)\n",
    "#clustering_linear = hierarchical_clustering_sklearn(linearly_transformed_df,n_clusters=n_clusters,linkage=linkage)\n",
    "#clustering_softmax = hierarchical_clustering_sklearn(softmax_scaled_df,n_clusters=n_clusters,linkage=linkage)\n",
    "\n",
    "# Self Organizing Map Grid Creation (Using Linear + Softmax Transformations)\n",
    "#som_linear = self_organizing_map(linearly_transformed_df,normalization=normalization,initialization=initialization,n_job=n_job,train_rough_len=train_rough_len,train_finetune_len=train_finetune_len,verbose=verbose)\n",
    "#som_softmax = self_organizing_map(softmax_scaled_df,normalization=normalization,initialization=initialization,n_job=n_job,train_rough_len=train_rough_len,train_finetune_len=train_finetune_len,verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Clusters\n",
    "\n",
    "There are many methods we can choose to visualize clusters:\n",
    "  - Dendrograms\n",
    "  - Elbow Curves\n",
    "  - SOMs\n",
    "  - Simply grouping by cluster and showing an aggregate value (mean, median, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@masarudheena/4-best-ways-to-find-optimal-number-of-clusters-for-clustering-with-python-code-706199fa957c\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "# https://medium.com/@sametgirgin/hierarchical-clustering-model-in-5-steps-with-python-6c45087d4318\n",
    "# https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def concat_data_and_clusters(X,features,labels):\n",
    "    subject_ids_and_cluster = pd.concat([pd.Series(features.index),pd.Series(labels)],axis=1)\n",
    "    subject_ids_and_cluster.columns = ['SUBJECT_ID','CLUSTER']\n",
    "    x_with_clusters = pd.merge(X,user_ids_and_cluster,how='inner',on='SUBJECT_ID')\n",
    "    return x_with_clusters\n",
    "\n",
    "def visualize_cluster(original_time_series_data,time_series_features_processed_df,clustering):\n",
    "    # https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "    x_with_clusters = concat_data_and_clusters(original_time_series_data,time_series_features_processed_df,clustering.labels_)\n",
    "    \n",
    "    # Create dictionaries for storing results/\n",
    "    clusters = {}\n",
    "    summaries = {}\n",
    "    histograms = {}\n",
    "    for cluster in x_with_clusters['CLUSTER'].unique():\n",
    "        current_cluster = x_with_clusters.loc[x_with_clusters['CLUSTER'] == cluster]\n",
    "        current_cluster_summary = current_cluster.describe()\n",
    "        current_cluster_histogram = current_cluster.hist(bins=50,figsize=(8,8))\n",
    "        print(current_cluster_summary)\n",
    "        clusters[cluster] = current_cluster\n",
    "        summaries[cluster] = current_cluster_summary\n",
    "        histograms[cluster] = current_cluster_histogram\n",
    "\n",
    "def plot_data_with_clusters(original_time_series_data,time_series_features_processed_df,clustering,analysis_column,date_col):\n",
    "    x_with_clusters = concat_data_and_clusters(original_time_series_data,time_series_features_processed_df,clustering.labels_)\n",
    "    \n",
    "    # According to docs, can plot labeled data this way if the labels have the column name 'y'\n",
    "    '''\n",
    "    x_with_clusters['y'] = x_with_clusters['CLUSTER']\n",
    "    x_col = 'DATE_COL'\n",
    "    y_col = analysis_column[0]\n",
    "    plt.plot(x_col,y_col,data=x_with_clusters)\n",
    "    plt.show()\n",
    "    '''\n",
    "    x = x_with_clusters[analysis_column]\n",
    "    dates = x_with_clusters[[date_col]]\n",
    "    labels = x_with_clusters['CLUSTER']\n",
    "    all_colors = ['red','green','blue','orange','purple']\n",
    "    colors = all_colors[:len(set(labels))]\n",
    "    #plt.scatter(x,y,c=labels)\n",
    "    plt.scatter(dates.values.ravel().tolist(),x.values.ravel().tolist(),c=labels.values.ravel(),cmap=plt.cm.Spectral)\n",
    "    plt.show()\n",
    "    '''\n",
    "    xi = x_with_clusters[['DATE_COL']]\n",
    "    yi = x_with_clusters[analysis_col]\n",
    "    labels = x_with_clusters[['CLUSTER']]\n",
    "    plt.plot(xi, yi, labels=labels)\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    by_label = tmp_df.groupby('CLUSTER')\n",
    "    for name, group in by_label:\n",
    "        plt.plot(group['DATE_COL'], group[analysis_col[0]], label=name)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    '''\n",
    "\n",
    "def plot_2d_clusters(original_time_series_data,time_series_features_processed_df,clustering,columns=[]):\n",
    "    x_with_clusters = concat_data_and_clusters(original_time_series_data,time_series_features_processed_df,clustering.labels_)\n",
    "    \n",
    "    if columns:\n",
    "        #x_subset = x_with_clusters[['USER_ID','CLUSTER'] + columns]\n",
    "        # https://stackoverflow.com/questions/42056713/matplotlib-scatterplot-with-legend\n",
    "        x = x_with_clusters[columns[0]]\n",
    "        y = x_with_clusters[columns[1]]\n",
    "        unique = list(set(labels))\n",
    "        colors = [plt.cm.jet(float(i)/max(unique)) for i in unique]\n",
    "        for i, u in enumerate(unique):\n",
    "            xi = [x[j] for j  in range(len(x)-1) if labels[j] == u]\n",
    "            yi = [y[j] for j  in range(len(x)-1) if labels[j] == u]\n",
    "            plt.scatter(xi, yi, c=colors[i], label=str(u))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    '''\n",
    "    \n",
    "    x=[1,2,3,4]\n",
    "    y=[5,6,7,8]\n",
    "    classes = [2,4,4,2]\n",
    "    unique = list(set(classes))\n",
    "    colors = [plt.cm.jet(float(i)/max(unique)) for i in unique]\n",
    "    for i, u in enumerate(unique):\n",
    "        xi = [x[j] for j  in range(len(x)) if classes[j] == u]\n",
    "        yi = [y[j] for j  in range(len(x)) if classes[j] == u]\n",
    "        plt.scatter(xi, yi, c=colors[i], label=str(u))\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    '''\n",
    "def plot_cluster_aggregate_values(original_df,transformed_df,cluster_labels,date_col):\n",
    "    x_with_clusters = concat_data_and_clusters(original_df,transformed_df,cluster_labels)\n",
    "    by_label_model = x_with_clusters.groupby('CLUSTER')\n",
    "    for name, group in by_label_model:\n",
    "        by_activity_date = group.groupby(date_col)\n",
    "        cluster_aggs = {}\n",
    "        for date, inner_group in by_activity_date:\n",
    "            cluster_mean = inner_group[analysis_col].median()\n",
    "            cluster_aggs[date] = cluster_mean\n",
    "        plt.plot(list(cluster_aggs.keys()),list(cluster_aggs.values()),label='Softmax cluster %s' % str(name))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_elbow_curve(X,som,method,K_values=range(1,10)):\n",
    "    # Method via https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/\n",
    "    # Takes a SOM grid, trains a K Means clustering algorithm using the different K_values\n",
    "    distortions = [] \n",
    "    inertias = [] \n",
    "    mapping1 = {} \n",
    "    mapping2 = {} \n",
    "    for k in K_values: \n",
    "        # Training a KMeans clustering model using the transformed SOM grid\n",
    "        kMeansClustering = som_kmeans_clustering(som,k=k)\n",
    "        # Save distortion and inertia values\n",
    "        distortions.append(sum(np.min(cdist(X, kMeansClustering.cluster_centers_, \n",
    "                          'euclidean'),axis=1)) / X.shape[0]) \n",
    "        inertias.append(kMeansClustering.inertia_) \n",
    "\n",
    "        mapping1[k] = sum(np.min(cdist(X, kMeansClustering.cluster_centers_, \n",
    "                     'euclidean'),axis=1)) / X.shape[0] \n",
    "        mapping2[k] = kMeansClustering.inertia_\n",
    "        \n",
    "        # Save SSE values\n",
    "        # Compute the L2-norm of the vector difference between each element in cluster n and cluster n’s centroid, and add this to the total SSE\n",
    "        # The L2 norm that is calculated as the square root of the sum of the squared vector values\n",
    "        \n",
    "        ### ADD CODE FOR SSE CALCULATION HERE ###\n",
    "    \n",
    "    # Plot elbow curve according to supplied method (either 'distortion' or 'inertia')\n",
    "    if method == 'inertia':\n",
    "        plt.plot(K_values, inertias, 'bx-') \n",
    "        plt.xlabel('Values of K') \n",
    "        plt.ylabel('Inertia') \n",
    "        plt.title('The Elbow Method using Inertia') \n",
    "    if method == 'distortion':\n",
    "        plt.plot(K_values, distortions, 'bx-') \n",
    "        plt.xlabel('Values of K') \n",
    "        plt.ylabel('Distortion') \n",
    "        plt.title('The Elbow Method using Distortion') \n",
    "    if method == 'sse':\n",
    "        pass\n",
    "        \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
