{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wearable Stress and Affect Detection (WESAD) Exploratory Data Analysis\n",
    "## https://archive.ics.uci.edu/ml/datasets/WESAD+%28Wearable+Stress+and+Affect+Detection%29#\n",
    "\n",
    "'''\n",
    "Philip Schmidt, Attila Reiss, Robert Duerichen, Claus Marberger and Kristof Van Laerhoven. 2018. \n",
    "Introducing WESAD, a multimodal dataset for Wearable Stress and Affect Detection. \n",
    "In 2018 International Conference on Multimodal Interaction (ICMI '18), October 16-20, 2018, Boulder, CO, USA. ACM, New York, NY, USA, 9 pages.\n",
    "'''\n",
    "\n",
    "This dataset is part of the UCI ML Data repository and contains high granularity data (700 Hz) of 15 test subjects from chest worn sensors (RespiBAN) in the form of:\n",
    "  - ECG\n",
    "  - EDA\n",
    "  - EMG\n",
    "  - Body Temp\n",
    "  - Accelorometer\n",
    "  - Respiration %\n",
    "  \n",
    "Contains data at lower granularity from wrist worn (non dominant) Empatica device in the form of:\n",
    "  - ACC\n",
    "  - BVP\n",
    "  - EDA\n",
    "  - Body Temp\n",
    "\n",
    "Wearable data generation has exploded in recent years, and with it the analysis of it. Time series data can yield very interesting insights and can paint a picture of people's health that they would not be able to see themselves.\n",
    "\n",
    "Let's explore this data and see what we find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this first so we can have plots in the cells\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Solarize_Light2',\n",
       " '_classic_test_patch',\n",
       " 'bmh',\n",
       " 'classic',\n",
       " 'dark_background',\n",
       " 'fast',\n",
       " 'fivethirtyeight',\n",
       " 'ggplot',\n",
       " 'grayscale',\n",
       " 'seaborn',\n",
       " 'seaborn-bright',\n",
       " 'seaborn-colorblind',\n",
       " 'seaborn-dark',\n",
       " 'seaborn-dark-palette',\n",
       " 'seaborn-darkgrid',\n",
       " 'seaborn-deep',\n",
       " 'seaborn-muted',\n",
       " 'seaborn-notebook',\n",
       " 'seaborn-paper',\n",
       " 'seaborn-pastel',\n",
       " 'seaborn-poster',\n",
       " 'seaborn-talk',\n",
       " 'seaborn-ticks',\n",
       " 'seaborn-white',\n",
       " 'seaborn-whitegrid',\n",
       " 'tableau-colorblind10']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Before we start, let's fix the row/column displays so we can see all rows/columns\n",
    "# Set ipython's max row display\n",
    "pd.set_option('display.max_row', 100)\n",
    "\n",
    "# Set iPython's max column width to 50\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# Also for numpy, since we will be creating arrays off of the data\n",
    "import numpy as np\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# Suppress scientific notation, show as decimals\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Do this to have jupyter notebook displayed in FULL\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Data Inspection\n",
    "\n",
    "Since we are working with wearables data it is reasonable to assume that the data we will be working with is largely continuous, with the labels being the one obvious discrete variable (which will require some additional analysis). \n",
    "\n",
    "For all variables let's check out:\n",
    " - The shape of the data (number of rows and columns)\n",
    " - .head() returns the first 5 rows of my dataset. This is useful if you want to see some example values for each variable.\n",
    " - .columns returns the name of all of your columns in the dataset.\n",
    "\n",
    "For the discrete \"label\" variable, we will look at:\n",
    "  - The unique values and their frequencies\n",
    "  \n",
    "Later on we will plot out our data to see how it varies over time. Since this was an experiment I am predicting we will see spikes in certain measurements related to heart/respiratory rate during experiments designed to elicit emotional responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../data/WESAD/S5/S5.pkl', '../../data/WESAD/S2/S2.pkl', '../../data/WESAD/S3/S3.pkl', '../../data/WESAD/S4/S4.pkl', '../../data/WESAD/S17/S17.pkl', '../../data/WESAD/S10/S10.pkl', '../../data/WESAD/S11/S11.pkl', '../../data/WESAD/S16/S16.pkl', '../../data/WESAD/S8/S8.pkl', '../../data/WESAD/S6/S6.pkl', '../../data/WESAD/S7/S7.pkl', '../../data/WESAD/S9/S9.pkl', '../../data/WESAD/S13/S13.pkl', '../../data/WESAD/S14/S14.pkl', '../../data/WESAD/S15/S15.pkl']\n"
     ]
    }
   ],
   "source": [
    "# Read in WESAD pickle files\n",
    "subject_dct = {}\n",
    "path = '../../data/WESAD'\n",
    "filenames = glob.glob(os.path.join(path,'*/*.pkl'))\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2\n"
     ]
    }
   ],
   "source": [
    "# Choose random subject to inspect \n",
    "# Since there were two participants (S1 & S12) whose data had to be removed, pick another number if we get those two\n",
    "from random import randint\n",
    "removed_subjects = [1,12]\n",
    "valid = False\n",
    "while not valid:\n",
    "    rand = randint(1,17)\n",
    "    if rand not in removed_subjects:\n",
    "        valid = True\n",
    "    else:\n",
    "        print('Picked removed subject',rand,', trying again')\n",
    "\n",
    "rand_id = 'S' + str(rand)\n",
    "print(rand_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/WESAD/S2/S2.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for corrresponding pickle file\n",
    "random_subject_file = [file for file in filenames if rand_id in file][0]\n",
    "print(str(random_subject_file))\n",
    "# Had to use 'latin1' as the encoding due to Python 2/3 pickle incompatibility\n",
    "# https://stackoverflow.com/questions/11305790/pickle-incompatibility-of-numpy-arrays-between-python-2-and-3\n",
    "# unpickled_file = pickle.load(open(file,'rb'), encoding='latin1')\n",
    "with open(random_subject_file,'rb') as unpickled_file:\n",
    "    # Take a look at what lies inside...\n",
    "    print(pickle.load(unpickled_file, encoding='latin1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'S2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bae3e1020022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now let's display shape, columns and heads of each df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Chest data (using \"display\" instead of print for head because it's formatted nicer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shape of chest data: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_dct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chest_df'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Columns of chest data:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubject_dct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chest_df'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Head of chest data:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'S2'"
     ]
    }
   ],
   "source": [
    "# Now let's display shape, columns and heads of each df\n",
    "# Chest data (using \"display\" instead of print for head because it's formatted nicer)\n",
    "print('Shape of chest data: {}'.format(subject_dct[rand_id]['chest_df'].shape))\n",
    "print('Columns of chest data:',subject_dct[rand_id]['chest_df'].columns)\n",
    "print('Head of chest data:')\n",
    "display(subject_dct[rand_id]['chest_df'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrist data\n",
    "for key,df in subject_dct[rand_id]['wrist_dfs'].items():\n",
    "    print('Shape of {} dataframe:'.format(key),df.shape)\n",
    "    print('Columns of {} dataframe:'.format(key),df.columns)\n",
    "    print('Head of {} dataframe:'.format(key))\n",
    "    print(df.head(10))\n",
    "#display(subject_dct[rand_id]['label_df'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess & Clean Data\n",
    "\n",
    "Now that we know more about our data, I will go ahead and get each subjects' data group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from the data_etl.py script\n",
    "\"\"\"\n",
    "According to the README:\n",
    "The double-tap signal pattern was used to manually synchronise the two devices' raw data. The result is provided in the files SX.pkl, one file per subject. This file is a dictionary, with the following keys:\n",
    "- 'subject': SX, the subject ID\n",
    "- 'signal': includes all the raw data, in two fields:\n",
    "  - 'chest': RespiBAN data (all the modalities: ACC, ECG, EDA, EMG, RESP, TEMP)\n",
    "  - 'wrist':EmpaticaE4data(all the modalities:ACC,BVP,EDA,TEMP)\n",
    "- 'label': ID of the respective study protocol condition, sampled at 700 Hz. The following IDs\n",
    "are provided: 0 = not defined / transient, 1 = baseline, 2 = stress, 3 = amusement, 4 = meditation, 5/6/7 = should be ignored in this dataset\n",
    "\"\"\"\n",
    "\n",
    "# Study protocal conditions (label) mapping\n",
    "label_map = {\n",
    "    0: 'not defined / transient',\n",
    "    1: 'baseline',\n",
    "    2: 'stress',\n",
    "    3: 'amusement',\n",
    "    4: 'meditation',\n",
    "}\n",
    "\n",
    "\n",
    "# Read in WESAD datasets by subject and unpickle\n",
    "subject_dct = {}\n",
    "path = '../../data/WESAD'\n",
    "filenames = glob.glob(os.path.join(path,'*/*.pkl'))\n",
    "for file in filenames:\n",
    "    # Had to use 'latin1' as the encoding due to Python 2/3 pickle incompatibility\n",
    "    # https://stackoverflow.com/questions/11305790/pickle-incompatibility-of-numpy-arrays-between-python-2-and-3\n",
    "    unpickled_file = pickle.load(open(file,'rb'), encoding='latin1')\n",
    "    # Grab relevant info\n",
    "    subject_id = unpickled_file['subject']\n",
    "    print('processing subject',subject_id)\n",
    "    chest_dct = unpickled_file['signal']['chest']\n",
    "    wrist_dct = unpickled_file['signal']['wrist']\n",
    "\n",
    "    # Process the chest dictionary first as it is more straight forward\n",
    "    # Since the 'ACC' column contains 3 dimensional tuples, it needs to be processed separately due to pandas expecting the same format for all columns\n",
    "    # Going to create dictionaries without that column to turn into a dataframe, then add the 'ACC' values later\n",
    "    tmp_chest_dct = dict((k, chest_dct[k].ravel()) for k in list(chest_dct.keys()) if k not in ['ACC'])\n",
    "    tmp_chest_df = pd.DataFrame(tmp_chest_dct) # Contains everything except ACC\n",
    "    tmp_acc_df = pd.DataFrame(chest_dct['ACC'],columns=['ACC_X','ACC_Y','ACC_Z']) # Manually declare keys, otherwise shows up as 0,1,2\n",
    "    final_chest_df = pd.concat([tmp_chest_df,tmp_acc_df],axis=1)\n",
    "\n",
    "    # Process wrist dictionary, which will take more care because the samplying frequencies were different \n",
    "    # Meaning the number of data points collected for each feature is different (higher frequency equals more data points)\n",
    "    # Basically this one just needs to be processed manually\n",
    "    wrist_acc_df = pd.DataFrame(wrist_dct['ACC'],columns=['ACC_X','ACC_Y','ACC_Z'])\n",
    "    wrist_bvp_df = pd.DataFrame(wrist_dct['BVP'],columns=['BVP'])\n",
    "    wrist_eda_df = pd.DataFrame(wrist_dct['EDA'],columns=['EDA'])\n",
    "    wrist_temp_df = pd.DataFrame(wrist_dct['TEMP'],columns=['TEMP'])\n",
    "\n",
    "    # Add labels as a separate object to be returned\n",
    "    # While the time granularity is the same as the chest data, I'm not sure yet how to use it with the wrist data\n",
    "    # So will just keep it separate and add as needed\n",
    "    labels_df = pd.DataFrame(unpickled_file['label'],columns=['label'])\n",
    "    labels_df['mapped_label'] = labels_df['label'].map(label_map)\n",
    "    labels_df['SUBJECT_ID'] = subject_id\n",
    "    \n",
    "    # Add subject id to all dataframes\n",
    "    for df in [final_chest_df, wrist_acc_df, wrist_bvp_df, wrist_eda_df, wrist_temp_df]:\n",
    "        df['SUBJECT_ID'] = subject_id\n",
    "\n",
    "    subject_dct[subject_id] = {\n",
    "        'chest_df': final_chest_df,\n",
    "        'wrist_dfs': {\n",
    "            'wrist_acc_df': wrist_acc_df,\n",
    "            'wrist_bvp_df': wrist_bvp_df,\n",
    "            'wrist_eda_df': wrist_eda_df,\n",
    "            'wrist_temp_df': wrist_temp_df,\n",
    "        },\n",
    "        'labels': labels_df,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Insights Using Pandas DataFrame .describe() Method\n",
    "The built in describe() method in Pandas dataframes is a great way to get an initial idea of the distributions of the numeric features in your data. For example, we can quickly get an idea whether the data is skewed or not by comparing the mean to the median (or 50th percentile). By looking at the other percentiles (25%, 75%), we can also see HOW skewed the data is in either direction.\n",
    "\n",
    "Note that I will do this by combining all data and describing it in aggregate; we will lose information this way because we are removing the time series element. We will revisit this later.\n",
    "\n",
    "Let's give it a go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chest Monitor Data\n",
    "\n",
    "We need to concatenate all data together to do this; its been a pain trying to do this due to all the data that's required to be joined.\n",
    "\n",
    "Found this [article](https://towardsdatascience.com/speeding-up-pandas-dataframe-concatenation-748fe237244e) that describes how the pandas concatenate function is very costly and uses the index to do the join. By dropping the index the join becomes much faster.\n",
    "\n",
    "Will demonstrate this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids = list(subject_dct.keys())\n",
    "display(subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First do without dropping index\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "'''\n",
    "chest_data = []\n",
    "label_data = []\n",
    "for subject_id in subject_ids:\n",
    "    chest_data.append(subject_dct[subject_id]['chest_df'])\n",
    "    label_data.append(subject_dct[subject_id]['labels']['label'])\n",
    "all_subjects_chest_data_df = pd.concat(chest_data)\n",
    "all_subjects_labels_df = pd.concat(label_data)\n",
    "all_subjects_chest_df = pd.concat([all_subjects_chest_data_df.reset_index(drop=True),all_subjects_labels_df.reset_index(drop=True)],axis=1)\n",
    "'''\n",
    "\n",
    "all_subjects_chest_data_df = pd.concat([subject_dct[subject_id]['chest_df'] for subject_id in list(subject_dct.keys())])\n",
    "all_subjects_labels_df = pd.concat([subject_dct[subject_id]['labels']['label'] for subject_id in list(subject_dct.keys())])\n",
    "all_subjects_chest_df = pd.concat([all_subjects_chest_data_df,all_subjects_labels_df],axis=1)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "print('Joining without dropping the index first took', total, 'seconds to complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subjects_chest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the join with dropping the index before concatenating \n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "'''\n",
    "chest_data = []\n",
    "label_data = []\n",
    "for subject_id in subject_ids:\n",
    "    chest_data.append(subject_dct[subject_id]['chest_df'])\n",
    "    label_data.append(subject_dct[subject_id]['labels']['label'])\n",
    "all_subjects_chest_data_df = pd.concat(chest_data)\n",
    "all_subjects_labels_df = pd.concat(label_data)\n",
    "all_subjects_chest_df = pd.concat([all_subjects_chest_data_df.reset_index(drop=True),all_subjects_labels_df.reset_index(drop=True)],axis=1)\n",
    "'''\n",
    "\n",
    "all_subjects_chest_data_df = pd.concat([subject_dct[subject_id]['chest_df'] for subject_id in list(subject_dct.keys())])\n",
    "all_subjects_labels_df = pd.concat([subject_dct[subject_id]['labels']['label'] for subject_id in list(subject_dct.keys())])\n",
    "all_subjects_chest_df = pd.concat([all_subjects_chest_data_df.reset_index(drop=True),all_subjects_labels_df.reset_index(drop=True)],axis=1)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "print('Joining with dropping the first index took', total, 'seconds to complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subjects_chest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe data (drop label since summary statistics are meaningless for it) & suppress scientific notation\n",
    "descriptive_stats = all_subjects_chest_df.drop(columns='label').describe().apply(lambda s: s.apply(lambda x: format(x, 'f'))) \n",
    "descriptive_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice! Now can start getting an idea of what the numeric data looks like\n",
    "# However, there are quite a bit of statistics here\n",
    "# Let's first look at the mean, medians and percentiles\n",
    "descriptive_stats.loc[['mean','std','25%','50%','75%']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrist Data (Needs to be Done Separately Due to Different Data Granularities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what keys are in the wrist_df\n",
    "subject_dct['S5']['wrist_dfs'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data together and describe\n",
    "all_subjects_wrist_acc_df = pd.concat([subject_dct[subject_id]['wrist_dfs']['wrist_acc_df'] for subject_id in list(subject_dct.keys())],axis=0)\n",
    "all_subjects_wrist_bvp_df = pd.concat([subject_dct[subject_id]['wrist_dfs']['wrist_bvp_df'] for subject_id in list(subject_dct.keys())],axis=0)\n",
    "all_subjects_wrist_eda_df = pd.concat([subject_dct[subject_id]['wrist_dfs']['wrist_eda_df'] for subject_id in list(subject_dct.keys())],axis=0)\n",
    "all_subjects_wrist_temp_df = pd.concat([subject_dct[subject_id]['wrist_dfs']['wrist_temp_df'] for subject_id in list(subject_dct.keys())],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df.describe().apply(lambda s: s.apply(lambda x: format(x, 'f'))) for df in [all_subjects_wrist_acc_df,all_subjects_wrist_bvp_df,all_subjects_wrist_eda_df,all_subjects_wrist_temp_df]],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Closer Look\n",
    "\n",
    "## Histograms\n",
    "If we really want to see how the values of a given feature are spread out and concentrated, we should plot the counts of each unique data point in each feature set.\n",
    "\n",
    "To do this pythonically (and to cut down on unnecessary code), we will define functions that will get unique value counts of a feature passed as an input, and then use those counts and plot a quick and dirty histogram.\n",
    "\n",
    "This way we can separately get a list of counts and analyze it by itself, as well as not have to create a ton of variables storing unique value counts of various features.\n",
    "\n",
    "**Note: This is a great starting point to analyze numerical categorical data (i.e. labels)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study protocal conditions (label) mapping\n",
    "label_map = {\n",
    "    0: 'not defined / transient',\n",
    "    1: 'baseline',\n",
    "    2: 'stress',\n",
    "    3: 'amusement',\n",
    "    4: 'meditation',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subjects_chest_df.hist(figsize=(20,20),bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subjects_wrist_acc_df.hist(figsize=(15,15),bins=50)\n",
    "all_subjects_wrist_bvp_df.hist(figsize=(15,15),bins=50)\n",
    "all_subjects_wrist_eda_df.hist(figsize=(15,15),bins=50)\n",
    "all_subjects_wrist_temp_df.hist(figsize=(15,15),bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Observations on the Sensor Data\n",
    "\n",
    "## Chest DataFrame\n",
    "\n",
    "- Electrocardiography (ECG): \n",
    "  - Nothing too much here, low average values, standard deviation and interquartile range. \n",
    "  - This is a measure of heart rate variability, so this suggests that for the most part there was no abormal electrical activity in the heart (we'll look into this more later)\n",
    "  - **This should be validated by a corresponding low variance in the respitory rate data since heart rate and breathing rate are highly linked**\n",
    "- Electromyography (EMG): \n",
    "  - Nothing much here at all: low average values, standard deviation and interquartile range\n",
    "- Electrodermal Activity (EDA): \n",
    "  - EDA is defined as \"transient change in certain electrical properties of the skin, resulting from sweat secretion and sweat gland activity\"\n",
    "  - Here we start to see more variation in the data from the other sensors, although it appears that the data fit between 1 and 10. \n",
    "  - This is likely due to the intended exercises in the experiment designed to elicit emotional reponses from the subjects. \n",
    "    - There was probably one experiment that was designed to elicit a huge response, while the others were for moderate responses\n",
    "- Accelerometer (ACC): \n",
    "  - Can't tell too much from just the descriptive statistics\n",
    "    - There was hardly any variation in the data points (low standard deviation and inter quartile range), so the minimum and maximum values were likely outliers. \n",
    "  - This should make sense too because this sensor is on the chest so it therefore won't move too much in any one direction unless the participants were all over the place.\n",
    "- Body Temperature (Celsius):\n",
    "  - Like the first set of sensors, the variation here is very low and there isn't too much to conclude other than the body temperature of the participants did not seem to vary that much (~33-34 degrees Celsius)\n",
    "    - The minimum value of -273 does suggest an error in data collection (_or someone's heart is just that cold??_) since this corresponds to absolute zero and the range of acceptable values was 0 - 50 according to the linked datasheets in the paper\n",
    "- Respitory %: \n",
    "  - Healthy amount of variation here; still largely normally distributed  w/ relatively low standard deviation and interquartile range\n",
    "      - **For the most part validates the observations around heart rate data above**\n",
    "  - There were likely experiments designed to elicit emotional responses, so the values at the tails likely represent this\n",
    "- Labels:\n",
    "  - Order of class by decreasing frequency as follows  (5,6, and 7 should be ignored):\n",
    "    1. 'not defined/transient'\n",
    "    2. 'baseline'\n",
    "    3. 'meditation'\n",
    "    4. 'stress', \n",
    "    5. 'amusement'\n",
    "  - This is mostly as expected, but looks like the state of meditation is quite frequenct so that is worth a closer look\n",
    "  \n",
    "## Wrist DataFrame\n",
    " \n",
    "- Accelerometer (ACC):\n",
    "  - Much more variation here, likely attributable to being on the wrist and being more prone to large changes in position\n",
    "      - Worth pointing out that the X direction data is concentrated at the ends while the Y & Z direction data is more normally distributed\n",
    "        - This means alot of movement was done in one set of directions (i.e. north to south, east to west, etc.)\n",
    "- BVP (measures from PPG):\n",
    "  - High overall range of values but graph shows a fairly normal distribution; min and max are likely outliers\n",
    "  - Don't have enough info on this data type to make any more meaningful conclusions\n",
    "- Electrodermal Activity (EDA): \n",
    "  - We see good variation here but less than in the chest data\n",
    "    - This is likely due to the fact that the chest worn sensor is more sensitive and will pick up smaller changes in electrical activity on the skin\n",
    "- Body Temperature:\n",
    "  - Slightly more variation here than the chest data and centered around lower temperature values, but still largely a tight range of numbers (no funky absolute zeros here)\n",
    "    - Perhaps since the chest sensor is under a shirt while the wrist device is out in the open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some overall assumptions\n",
    "\n",
    "Based on the above observations let's make a couple assumptions about the data and use other methods to try and prove them wrong:\n",
    "1. Most of the high respitory rate data occurred during a specific range of values (i.e. during a specific stress experiment).\n",
    "2. Higher EDA values correlate highly (?) to labels of 'stress' and 'amusement' (and lower correspond to 'meditation' and 'baseline').\n",
    "3. Most of the changes to the accelerometer data occur during a specific range of values (moving to different areas for experiments)\n",
    "4. The EDA wrist and EDA chest data are highly correlated per subject (all above 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's prove them wrong!!\n",
    "\n",
    "## Plot Data Directly\n",
    "\n",
    "In order to get the complete picture and test out the above assumptions, we should plot the data in order to see how it varies across the time period of the experiment. In order to be able to compare each subjects's data in the most intuitive way we will plot all subject data onto one graph. This will allow us to validate or discard some of the early conclusions made above.\n",
    "\n",
    "**Note: We can do this because we have a limited number of actual subjects in the data; if there were hundreds it wouldn't be practical to graph each subject's data on one graph (instead it would be more appropriate to graph summary statistics)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chest Data + Labels\n",
    "\n",
    "We're going to have to plot these one by one due to the sheer amount of data for each category. Even the plots below take around half a minute each just to render properly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for subject in subject_ids:\n",
    "    subject_dct[subject]['chest_df']['EDA'].plot(figsize = (20,15), label = subject)\n",
    "\n",
    "ax.legend(subject_ids)\n",
    "ax.set_title('EDA by Subject')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for subject in subject_ids:\n",
    "    subject_dct[subject]['labels']['label'].plot(figsize = (20,15))\n",
    "\n",
    "ax.legend(subject_ids)\n",
    "ax.set_title('Labels by Subject')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for subject in subject_ids:\n",
    "    subject_dct[subject]['chest_df']['ECG'].plot(figsize = (20,15))\n",
    "\n",
    "ax.legend(subject_ids)\n",
    "ax.set_title('ECG by Subject')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for subject in subject_ids:\n",
    "    subject_dct[subject]['chest_df']['EMG'].plot(figsize = (20,15))\n",
    "\n",
    "ax.legend(subject_ids)\n",
    "ax.set_title('EMG by Subject')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for subject in subject_ids:\n",
    "    subject_dct[subject]['chest_df']['ACC_X'].plot(figsize = (20,15))\n",
    "\n",
    "ax.legend(subject_ids)\n",
    "ax.set_title('ACC_X by Subject')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for subject in subject_ids:\n",
    "    subject_dct[subject]['chest_df']['ACC_Y'].plot(figsize = (20,15))\n",
    "\n",
    "ax.legend(subject_ids)\n",
    "ax.set_title('ACC_Y by Subject')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for subject in subject_ids:\n",
    "    subject_dct[subject]['chest_df']['ACC_Z'].plot(figsize = (20,15))\n",
    "\n",
    "ax.legend(subject_ids)\n",
    "ax.set_title('ACC_Z by Subject')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for subject in subject_ids:\n",
    "    subject_dct[subject]['chest_df']['Temp'].plot(figsize = (20,15))\n",
    "\n",
    "ax.legend(subject_ids)\n",
    "ax.set_title('Temp by Subject')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for subject in subject_ids:\n",
    "    subject_dct[subject]['chest_df']['Resp'].plot(figsize = (20,15))\n",
    "\n",
    "ax.legend(subject_ids)\n",
    "ax.set_title('Resp by Subject')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrist Data\n",
    "\n",
    "Here we can go ahead and plot the data all at once and not have to wait too long for the data to render. We'll also be able to see all the graphs much closer together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrist_keys = list(subject_dct[subject]['wrist_dfs'].keys())\n",
    "fig,axs = plt.subplots(6,1,figsize=(50,40))\n",
    "for subject in subject_ids:\n",
    "    # For ACC, need to split out X,Y,Z\n",
    "    axs[0].plot(subject_dct[subject]['wrist_dfs'][wrist_keys[0]]['ACC_X'])\n",
    "    axs[0].set_title('ACC_X')\n",
    "    axs[1].plot(subject_dct[subject]['wrist_dfs'][wrist_keys[0]]['ACC_Y'])\n",
    "    axs[1].set_title('ACC_Y')\n",
    "    axs[2].plot(subject_dct[subject]['wrist_dfs'][wrist_keys[0]]['ACC_Z'])\n",
    "    axs[2].set_title('ACC_Z')\n",
    "    # Do the rest normally\n",
    "    axs[3].plot(subject_dct[subject]['wrist_dfs'][wrist_keys[1]].drop(columns='SUBJECT_ID'))\n",
    "    axs[3].set_title(wrist_keys[1].split('_')[1].upper())\n",
    "    axs[4].plot(subject_dct[subject]['wrist_dfs'][wrist_keys[2]].drop(columns='SUBJECT_ID'))\n",
    "    axs[4].set_title(wrist_keys[2].split('_')[1].upper())\n",
    "    axs[5].plot(subject_dct[subject]['wrist_dfs'][wrist_keys[3]].drop(columns='SUBJECT_ID'))\n",
    "    axs[5].set_title(wrist_keys[3].split('_')[1].upper())\n",
    "\n",
    "axs[0].legend(subject_ids)\n",
    "axs[1].legend(subject_ids)\n",
    "axs[2].legend(subject_ids)\n",
    "axs[3].legend(subject_ids)\n",
    "axs[4].legend(subject_ids)\n",
    "axs[5].legend(subject_ids)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter Plots\n",
    "\n",
    "While the above descriptive statistics are helpful, it may be less clear how features with similar measures (mean, median, standard deviation, etc.) relate to each other (if at all). We should graph these features against each other and see how they relate to each other (negatively/positively, strong/weak, etc.). \n",
    "\n",
    "If we end up deciding to try and create a model later, this analysis will help us decide what variables to use and remove.\n",
    "\n",
    "For this, we will import another visualization packages (seaborn) that has some nicer plotting features and functionalities.\n",
    "\n",
    "## Chest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package and plot correlation\n",
    "import seaborn as sns\n",
    "\n",
    "chest_corr = all_subjects_chest_df.corr()\n",
    "display(chest_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there isn't much strong correlation here, let's graph it for a better view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still alot of variables, but let's give it a go and see what happens\n",
    "# From https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec\n",
    "def plot_sns_heatmap(corr):\n",
    "    plt.figure(figsize=(14,14))\n",
    "    ax = sns.heatmap(\n",
    "        corr, \n",
    "        vmin=-1, vmax=1, center=0,\n",
    "        cmap=sns.diverging_palette(20, 220, n=200),\n",
    "        square=True\n",
    "    )\n",
    "    ax.set_xticklabels(\n",
    "        ax.get_xticklabels(),\n",
    "        rotation=45,\n",
    "        horizontalalignment='right'\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sns_heatmap(chest_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corresponding Chest + Wrist Data (i.e. Chest EDA vs Wrist EDA)\n",
    "\n",
    "Obviously comparing the chest and wrist data with different time granularities needs to be taken with a grain of salt but let's try it anyway. I will be doing the correlation on a subject by subject basis to see how the values corresponded.\n",
    "\n",
    "For this, I will perform some different aggregations to get the chest data onto the same granularity as the wrist. We will certainly lose some information here, but let's see what we find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and median of every N data points where N is the ratio of chest data points to wrist data points\n",
    "# Using same idea as here: https://stackoverflow.com/questions/47239332/take-the-sum-of-every-n-rows-in-a-pandas-series\n",
    "'''\n",
    "for subject in subject_ids:\n",
    "    ratio = subject_dct[subject]['chest_df']['EDA'].shape[0] / subject_dct[subject]['wrist_dfs']['wrist_eda_df'].shape[0]\n",
    "    print('Ratio for subject', subject, 'is', ratio)\n",
    "'''\n",
    "corr_dct = {}\n",
    "combined_dct = {}\n",
    "N = 175 # Validated by code above\n",
    "for subject in subject_ids:\n",
    "    \n",
    "    # Calculate mean/median for chest data\n",
    "    df = subject_dct[subject]['chest_df']['EDA']\n",
    "    mean = pd.DataFrame(df.groupby(df.index // N).mean()).rename(columns={'EDA': 'EDA_CHEST_MEAN'})\n",
    "    median = pd.DataFrame(df.groupby(df.index // N).median()).rename(columns={'EDA': 'EDA_CHEST_MEDIAN'})\n",
    "    combined = pd.concat([mean,median],axis=1)\n",
    "    combined_dct[subject] = combined\n",
    "    \n",
    "    # Calculate correlation with wrist data\n",
    "    chest_df = subject_dct[subject]['wrist_dfs']['wrist_eda_df'].rename(columns={'EDA': 'EDA_WRIST'})\n",
    "    all_data = pd.concat([chest_df,combined],axis=1)\n",
    "    corr_dct[subject] = all_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sns_heatmap(corr_dct['S4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "We appear to have "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
